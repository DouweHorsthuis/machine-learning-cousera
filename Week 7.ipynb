{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Specialization\n",
    "## Advanced learning Algorithms\n",
    "# Week 7  \n",
    "  \n",
    "## Terminology\n",
    "\n",
    "| Definition        | Explanation            |\n",
    "|-------------------|------------------------|\n",
    "|Decision tree model | Notes might lead you to a different next option |  \n",
    "|Root node| Starting point of a decision tree |  \n",
    "|leaf nodes | The nodes that makes the final prediction|  \n",
    "|Decision nodes | all the notes between the root note and the leaf notes |  \n",
    "|Maximize purity| Trying to get as much of the group you are looking for grouped together when splitting data |  \n",
    "|$p_1$| Fraction of examples that are whatever you want your data to find | \n",
    "| Entropy | Purity of your data |  \n",
    "|$H(p_1)$ | Entropy function |  \n",
    "|information gain | The reduction in entropy that you get in your tree, resulting from making a split |  \n",
    "|Recursive algorithm | Building a decision tree at the root followed by smaller decision tree sub-branches |  \n",
    "| Tree ensemble | Multiple decision trees that are all used | "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision tree model  \n",
    "Decision trees are often used for machine learning models. A decision tree has a starting point, the leaf note, but based on the the outcome of a node directs the data into a specific path. There can be multiple decision nodes but the end layer always has decision nodes, where the final outcome is decided."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The process of building a decision tree  \n",
    "The first feature you have to decide is what will be decided in the root node. You want to maximize purty here. This means find a way to \"catch\" as many of the group you are looking for. You then decide on the features on the left side of your decision tree. How can you split up this group. If you see that after that split, you end up with fully splitting up the group you were looking for from the group you didn't want, you have defined your decision node. After that you do the same for the right side of your tree. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decisions when making a decision tree  \n",
    "1. How to choose what feature to split on at each node.  \n",
    "    - You want to maximize purity at each node.\n",
    "2. When do you stop splitting  \n",
    "    - When a node is 100% one class\n",
    "    - When splitting a node will result in the tree exceeding a maximum depth you have set before\n",
    "    - When improvements in purity score are below a threshold \n",
    "    - When numbers of example in a node is below a threshold\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Entropy as a measure of impurity \n",
    "Purity can mean that you either have found 100% of the group you were looking for or 0%. The most impure would be 50%. It's denoted as $p_1$, which is a fraction of examples that your were looking for. The entropy function is noted as $H(p_1)$. So it's a number between 0-1. Where H = highest at a 50% purity and lowest at 0 or 100%. The function to caluclate $H(p_1)$ is:  \n",
    "$H(p_1) = -p_1log_2(p_1)-p_0log_2(p_0) = -p_1log_2(p_1)-(1-p_1)log_2(1-p_1)$  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Choosing a split  \n",
    "To give an example and work from there. Let's say we are looking for cats vs dogs and we have 3 possible splits with their entropy values:\n",
    "- Ear shapes(point vs floppy)\n",
    "    1. $p_1=\\frac{4}{5}=0.8$&nbsp; &nbsp; &nbsp;H(0.8)=0.72\n",
    "    2. $p_1=\\frac{1}{5}=0.2$&nbsp; &nbsp; &nbsp;H(0.2)=0.72\n",
    "- Face shapes (round vs not round)\n",
    "    1. $p_1=\\frac{4}{7}=0.57$&nbsp; &nbsp; &nbsp;H(0.57)=0.99\n",
    "    2. $p_1=\\frac{1}{3}=0.33$&nbsp; &nbsp; &nbsp;H(0.33)=0.92  \n",
    "- Whiskes (present vs absent)\n",
    "    1. $p_1=\\frac{3}{4}=0.75$&nbsp; &nbsp; &nbsp;H(0.75)=0.81\n",
    "    2. $p_1=\\frac{2}{6}=0.33$&nbsp; &nbsp; &nbsp;H(0.33)=0.92  \n",
    "  \n",
    "Based on this, you could say that you could simply go based on the best Entropy number (H). But actually *what you want to do is using a weighted average*. Because if you you have a node with a lot of examples with high entropy it's worse then a small group and high entropy. Because you have to think of it inside the structure of a branch of your decision tree. So if there is one branch with a lot of examples and it's very impure, that would have a big impact on your data, but if you end up with a very small impure branch it might actually not matter that much. You an calculate this weighted average the following way:  \n",
    "For the ear shapes: &nbsp; $(\\frac{5}{10}H(0.8)+\\frac{5}{10}H(0.2))$  \n",
    "For the face shapes: $(\\frac{7}{10}H(0.57)+\\frac{3}{10}H(0.33))$  \n",
    "For the whiskers:&nbsp; &nbsp;&nbsp; &nbsp;$(\\frac{4}{10}H(0.75)+\\frac{6}{10}H(0.33))$  \n",
    "  \n",
    "**Your final step is to take this weighted average from the entropy if we have hadn't split at all**. In this case we know we have 5/10 cats, so that means 0.5-the weighted average:  \n",
    "For the ear shapes: &nbsp; 0.28  \n",
    "For the face shapes: 0.03  \n",
    "For the whiskers:&nbsp; &nbsp;&nbsp; &nbsp;0.12  \n",
    "  \n",
    "This would mean we would split based on ear shapes because it's the highest number, so it would reduce the entropy the most.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Information gain\n",
    "As described above, we now know what information gain is and how we can calculate it. Here we will formalize it a little bit more:  \n",
    "![](https://github.com/DouweHorsthuis/machine-learning-cousera/blob/main/images/information_gain.PNG?raw=true)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Recursive splitting  \n",
    "All we have done, is create a recursive splitting algorithm. Because after every split we decide to run the same (recursive) step to decide on the next tree branch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### One-hot encoding  \n",
    "Sometimes you need to have more options for a branch. In this case case you use one-hot encoding. It is important to know that you need to be sure that your data can only take on, one of the possibilities. In our example above, we said the data could only have pointy of floppy ears. But maybe it's also possible that they have Round ears. In this case, you would are sure that for each of these catagories the dataset could only get a 1 for one of them, and a 0 for the other 2.  If that's the case, you are back to a place where you have a binary output. So you able to use it as you were before. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Continues value in a decision tree  \n",
    "Let's continue with cats and dogs. If we would add weight. We would have a continues number rather than only binary numbers. In this case you can use the information gain calculation to get the best number that splits the groups the best.  \n",
    "![](https://github.com/DouweHorsthuis/machine-learning-cousera/blob/main/images/splitting.PNG?raw=true)   \n",
    " \n",
    "Once you know the number, you have again a split point that can have a binary outcome (in the example bigger/small than 9lbs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predicting a number as outcome (regression problem) \n",
    "In this case, instead of reducing entropy. You want to reduce the amount of variance in your outcome of each split.  \n",
    "![](https://github.com/DouweHorsthuis/machine-learning-cousera/blob/main/images/continues_split.PNG?raw=true)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision tree ensemble  \n",
    "Often people don't use just one decision tree but multiple. This is called a decision tree ensemble. This is because, if we look at our previous example, we would chance just one dataset it *ear shape* would be replaced by *whiskers* as the first decision and lead to a very different tree.  \n",
    "Let's say we use 3 trees of which 2 tell us that our new dataset is a cat and 1 tells us it is not. We rely on the majority answer and go with cat.  **People recommend somewhere between 64 to 128 unique decision trees**. Going beyond a certain point (around 100-128 maybe) seems to not hurt, but give diminishing returns. So it's not worth it, and slowing everything down. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sampling with replacement  \n",
    "To create multiple trees, you need to use sampling with replacements. This means that if you have 10 original datasets and for each tree, you want to use 10 datasets. This would make them identical, unless you sample with replacements. This means that you pick randomly 1 dataset out of the 10. After that you choose another one, but you make sure that you could again pick the same one. Like this your decision tree has to work with different weighted features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The random forest algorithm  \n",
    "It seems to be the case that if you create lets say a 100 trees, they might all, or at least a lot of them, have the same decision at the start and make similar choices.  To prevent this, at each not, when choosing a feature to use the split, if $n$ features are available, pick a random subset of $k$<$n$ features and allow the algorithm to only choose from that subset of features. People often use $k=\\sqrt{n}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Boosted forest algorithm  \n",
    "Most used forest algorithm is XGBoost (open source). In the previous algorithms, we used sampling with replacement to create a new training set of size $m$. A boosted tree however does the same for the first loop, but after that, **instead of picking from all examples with equal 1/m) probability, it makes it more likely to pick examples that the previously trained trees misclassified.   \n",
    "\n",
    "\n",
    "**XGBoost Classification code**  \n",
    "```python\n",
    "from xgboost import XGBClassifier\n",
    "model=XGBClassifier()\n",
    "model.fit(X_train,Y_train)\n",
    "y_pred=mode.predict(X_test)\n",
    "```  \n",
    "**XGBoost Regression code**  \n",
    "```python\n",
    "from xgboost import XGBRegressor\n",
    "model=XGBRegressor()\n",
    "model.fit(X_train,Y_train)\n",
    "y_pred=mode.predict(X_test)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision tree vs Neural network  \n",
    "\n",
    "| | Decision Trees| Neural networks| \n",
    "|--|--|--|\n",
    "|Tabular (structured) data | Works well | Works well| \n",
    "| unstructured data (images, audio, text) | Not recommend | Works well |  \n",
    "| Learning speed | Fast | Slow| \n",
    "| Human interpretable | If they are small | Not really |\n",
    "| Transferred Learning | No | Yes| \n",
    "| Combining multiple models | Not really | yes | "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ba80823ec9edfa2b915b0e8b60f600e1ca4cb90dcbbf7f0d0e0246605aecf2a6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
