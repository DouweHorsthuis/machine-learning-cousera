{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Specialization\n",
    "## Supervised Machine Learning: Regression and Classification\n",
    "## Week 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Terminology\n",
    "\n",
    "| Definition        | Explanation            |\n",
    "|-------------------|------------------------|\n",
    "| Classification| A model where output Y can only take on 1 out of a small number of values |  \n",
    "| Binary classification | If y can only be one of two values | \n",
    "| Class or category | Often used interchangeably, they are the answers to the classification | \n",
    "|Answers Yes/True/1 | Positive class|\n",
    "|Answers No/False/0 | Negative class|  \n",
    "|Sigmoid Function or Logistic Function | g(z) = $\\frac{\\partial 1}{\\partial 1+e^{-z}}$ |\n",
    "| Loss * $loss(f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}), y^{(i)})$ | the difference of every example to it's target | \n",
    "| Cost | All the losses together over the dataset |  \n",
    "| Underfit / High bias | A model that does not fit the data well |\n",
    "| Generalization | How well the model works on new data | \n",
    "| Overfit / High variance | A model that fits the training data perfectly, but does not generalize well to new data |  \n",
    "| Regularization | Reducing a parameter but not setting it to 0 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification  \n",
    "Linear regression is not a good algorithm for this problem. Because you can get infinitive numbers for the outcome and you want only 2. Furthermore, even when you would get a useful output to create a boundary that would separate well, adding more data would very quickly prove it to not be good anymore.  The goal of classification is often answered by Yes or no, in this case you call it Binary classification.  \n",
    "  \n",
    "![Using linear regression to predict](https://github.com/DouweHorsthuis/machine-learning-cousera/blob/main/images/using_linear_reg_1.PNG?raw=true)  \n",
    "If you use linear regression to classify, you could us it to come up with a good threshold as seen here by the vertical blue line.  \n",
    "![The problem of using linear regression](https://github.com/DouweHorsthuis/machine-learning-cousera/blob/main/images/using_linear_reg_2.PNG?raw=true)  \n",
    "A big problem happens when another data point is added. If you would use the green line to classify, end up miss classifying data   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic regression\n",
    "Instead of a straight line (or any of the other linear options), logistic regression creates a kind of S shaped line. To calculate this one would use the **Sigmoid function** or logistic function. The sigmoid functions outputs values between 0 and 1. It has the formula g(z) = $\\frac{\\partial 1}{\\partial 1+e^{-z}}$. Another way people might write it is P(y=1|x;$\\overrightarrow{w}$,b). Here what they mean is the probability that y=1 given x with the parameters x and b. This should give you a number like 0.7 or 0.3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the formula z is equal to $\\overrightarrow{w} \\cdot \\overrightarrow{x} + b$. This means we can write the full formula as $ f_{\\overrightarrow{w},b} (\\overrightarrow{x}) $ = g($\\overrightarrow{w} \\cdot \\overrightarrow{x} + b)$ = g(z) = $\\frac{\\partial 1}{\\partial 1+e^{-z}}$ P(y=1|x;$\\overrightarrow{w}$,b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Threshold\n",
    "But how do you get a prediction if it's a 1 or a 0 (classifier). In this case you need to set a threshold. Since the outcome of the formula is a number between 0 and 1. 0.5 would make a lot of sense because this would be half way. But what would this mean.  \n",
    "$\\hat{y} $= 1 if  \n",
    "$ f_{\\overrightarrow{w},b} (\\overrightarrow{x}) \\le 0.5 $  \n",
    "g(z) $\\le 0.5 $  \n",
    "z $\\le 0.5 $  \n",
    "$\\overrightarrow{w} \\cdot \\overrightarrow{x} + b  \\le 0.5 $  \n",
    "\n",
    "However, while 0.5 makes sense when the data is split in a linear way, it might not make sense when the data is very complex. So ultimately finding a threshold should be based on your data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cost function for Logistic regression  \n",
    "### Why not use the squared error cost function\n",
    "We already went over the squared error cost function for linear regression. Where $f_{\\overrightarrow{w},b} (\\overrightarrow{x}) \\overrightarrow{w} \\cdot \\overrightarrow{x} + b $. This cost function would however not work for logistic regression. If we would try it for logistic regression the function would be:  \n",
    "$ f_{\\overrightarrow{w},b} (\\overrightarrow{x}) $ = $\\frac{\\ 1}{\\ 1+e^{-z}}$. This would be instead a non-convex function. Which would create a lot of local minima which would be a problem.  \n",
    "\n",
    "![The problem of using squared cost error for logistic regression](https://github.com/DouweHorsthuis/machine-learning-cousera/blob/main/images/square_cost_error_logistic_regression.PNG?raw=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss function\n",
    "The *loss* is the part of the cost function that comes after the sum:  \n",
    "*J($\\overrightarrow{w}$,b)=$\\frac{1}{m}$ $\\sum\\limits_{i=1}^{m}$ $\\frac{1}{2}$ ($f$<sub>$\\overrightarrow{w}$,b</sub>($\\overrightarrow{x}$<sup>(i)</sup>)-y<sup>(i)</sup>)<sup>2</sup>*.  \n",
    "To be clear, the original cost function is slightly different because we moved the 1 half inside the summation. This is to make the math easier later on. So from here on we will call loss:  \n",
    "L($f$<sub>$\\overrightarrow{w}$,b</sub>($\\overrightarrow{x}$<sup>(i)</sup>),y<sup>(i)</sup>)  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loss for logistic regression  \n",
    "Now that we know that the cost function doesn't work for logistic regression, we need to adjust it so that it will. For this we will specifically look at the loss part. Instead of using L($f$<sub>$\\overrightarrow{w}$,b</sub>($\\overrightarrow{x}$<sup>(i)</sup>),y<sup>(i)</sup>)  we need to re-define it.  \n",
    "To work we will actually have 2 versions, one for a prediction of $y^{(i)}=1$ and one when $y^{(i)}=0$  \n",
    "\n",
    "if $y^{(i)}=1$ we use   -log($f$<sub>$\\overrightarrow{w}$,b</sub>($\\overrightarrow{x}$<sup>(i)</sup>))  \n",
    "if $y^{(i)}=0$ we use   -log(1 - $f$<sub>$\\overrightarrow{w}$,b</sub>($\\overrightarrow{x}$<sup>(i)</sup>))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Simpliefied loss function  \n",
    "If we use the new loss part, the cost function will become convex again. The whole function would look like this:  \n",
    "$$loss(f_{\\mathbf{\\overrightarrow{w}},b}(\\mathbf{x}^{(i)}), y^{(i)}) = (-y^{(i)} \\log\\left(f_{\\mathbf{w},b}\\left( \\mathbf{x}^{(i)} \\right) \\right) - \\left( 1 - y^{(i)}\\right) \\log \\left( 1 - f_{\\mathbf{w},b}\\left( \\mathbf{x}^{(i)} \\right) \\right)$$  \n",
    "\n",
    "And to clarify, if $y^{(i)}$ = 0 the left hand side is eliminated, and if  $y^{(i)}$ = 1 the right hand side is eliminated. Which makes it a little easier to work with. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Simplified cost function  \n",
    "Just like we did with the loss function, we can also simplify the cost function to the one most people use:  \n",
    "$J(\\overrightarrow{w},b) = -\\frac{1}{m} \\sum_{i=0}^{m} \\left[ y^{(i)} \\log\\left(f_{{\\overrightarrow{x}},b}\\left( \\overrightarrow{x}^{(i)} \\right) \\right) + \\left( 1 - y^{(i)}\\right) \\log \\left( 1 - f_{\\overrightarrow{x},b}\\left( \\overrightarrow{x}^{(i)} \\right) \\right) \\right]$    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overfitting and underfitting\n",
    "What is overfitting or underfitting? Underfitting means that the model does not fit the data well, this means the the algorithm has a high bias. So to clarify, the algorithm has a bias that for example the data fits a straight line, while this is not true. Overfitting is the opposite, it means that you come up with a model that fit the training data perfect, but doesn't generalize well.  \n",
    "![Underfitting and overfitting](https://github.com/DouweHorsthuis/machine-learning-cousera/blob/main/images/overfit_underfit.PNG?raw=true)  \n",
    "On the left you see unbefitting while on the right it's a clear case of overfitting.  \n",
    "  \n",
    "## How to not overfit    \n",
    "The number one step is collecting more data to train you model. If you cannot get more data you might have to reduce your features. The 3 possibility is **regularization**. It is similar to step to, but instead you don't reduce or set the feature to 0, you just decrease it's size so the impact is smaller."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularization  \n",
    "Since we don't want to lose features that we think might be important, regularization is very often used. One of the issues is, which of your features should be changed? Especially when you have a lot of features (lets say 100), it would be very difficult to find out. So instead we apply regularization to all the features. You can do this by adding the following to your cost function:  \n",
    "$\\frac{\\lambda}{2m}\\sum_{j=0}^{n}w^2_j$. Which gives you the full model:  \n",
    "\n",
    "$J(\\overrightarrow{w},b)=\\frac{1}{2m}\\sum\\limits_{i=1}^{m}(f_{\\overrightarrow{w},b}(\\overrightarrow{x}^(i))-y^(i))^2 + \\frac{\\lambda}{2m}\\sum_{j=0}^{n}w^2_j$ \n",
    "  \n",
    "*Some people add $\\frac{\\lambda}{2m}b^2$ to the end. You can do this, but in practice this has very little effect and thus isn't done by everyone*  \n",
    "  \n",
    "### Impact of $\\lambda$  \n",
    "If you set lambda to be 0, it has no impact on your data. This means that if you have a very complex model, it wil still be really complex. However, if you set $\\lambda$ to a really high number, all your features will end up being close to 0. Which results in f(x)=b , and thus gives you a horizontal line. What you want, is a number for $\\lambda$ between these two extremes. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularization update \n",
    "### Linear regression \n",
    "As we done before, when we learn a new addition to gradient decent, we need to update the model. In the case for **Linear regression** we go from:  \n",
    "  \n",
    "$w_j=\\frac{1}{m}\\sum\\limits_{i=1}^{m}\\frac{1}{2}(f_{\\overrightarrow{w},b}(\\overrightarrow{x}^{(i)})-y^{(i)})x^{(i)}_j$  \n",
    "  \n",
    "$b=\\frac{1}{m}\\sum\\limits_{i=1}^{m}\\frac{1}{2}(f_{\\overrightarrow{w},b}(\\overrightarrow{x}^{(i)})-y^{(i)})$  \n",
    "*Which you update simultaneously*  \n",
    "  \n",
    "To:  \n",
    "$w_j=\\frac{1}{m}\\sum\\limits_{i=1}^{m}\\frac{1}{2}(f_{\\overrightarrow{w},b}(\\overrightarrow{x}^{(i)})-y^{(i)})x^{(i)}_j + $<ins>$\\frac{\\lambda}{m}w_j$</ins>  \n",
    "  \n",
    "$b=\\frac{1}{m}\\sum\\limits_{i=1}^{m}\\frac{1}{2}(f_{\\overrightarrow{w},b}(\\overrightarrow{x}^{(i)})-y^{(i)})$  \n",
    "*Which you still update simultaneously*  \n",
    "  \n",
    "### Logistic regression\n",
    "In the case of **logistic regression** we go from:  \n",
    "$J(\\overrightarrow{w},b) = -\\frac{1}{m} \\sum\\limits_{i=1}^{m} \\left[ y^{(i)} \\log\\left(f_{{\\overrightarrow{w}},b}\\left( \\overrightarrow{x}^{(i)} \\right) \\right) + \\left( 1 - y^{(i)}\\right) \\log \\left( 1 - f_{\\overrightarrow{x},b}\\left( \\overrightarrow{x}^{(i)} \\right) \\right) \\right]$  \n",
    "\n",
    "To:  \n",
    "$J(\\overrightarrow{w},b) = -\\frac{1}{m} \\sum\\limits_{i=1}^{m} \\left[ y^{(i)} \\log\\left(f_{{\\overrightarrow{w}},b}\\left( \\overrightarrow{x}^{(i)} \\right) \\right) + \\left( 1 - y^{(i)}\\right) \\log \\left( 1 - f_{\\overrightarrow{x},b}\\left( \\overrightarrow{x}^{(i)} \\right) \\right) \\right] $<ins>$\\frac{\\lambda}{m}w_j$</ins> $  \n",
    "  \n",
    "Which mean for the derivatives:  \n",
    "$w_j = \\frac{1}{m} \\sum\\limits_{i=1}^{m} [(f_{\\overrightarrow{w},b})(\\overrightarrow{x}^{(i)})-y^{(i)})x^{(i)}_j]+$<ins>$\\frac{\\lambda}{m}w_j$</ins>  \n",
    "  \n",
    "$b=\\frac{1}{m} \\sum\\limits_{i=1}^{m} [(f_{\\overrightarrow{w},b})(\\overrightarrow{x}^{(i)})-y^{(i)})$  \n",
    "*which both need to be updated simultaneously*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ba80823ec9edfa2b915b0e8b60f600e1ca4cb90dcbbf7f0d0e0246605aecf2a6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
