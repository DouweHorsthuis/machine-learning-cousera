{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Specialization\n",
    "## Unsupervised Learning, Recommenders, Reinforcement Learning\n",
    "# Week 9  \n",
    "  \n",
    "## Terminology\n",
    "\n",
    "| Definition        | Explanation            |\n",
    "|-------------------|------------------------|  \n",
    "| Recommender system | a system that will predict things you want |  \n",
    "| $n_u$ | no. of users | \n",
    "|$n_m$ | no. of movies |\n",
    "|$r(i,j)=1$ | notation if rating for movie i by user j (always a 0 or 1)|  \n",
    "|$y^{i,j}$| notation to find the rating given by user j for movie i (score) |  \n",
    "|$m^{(j)}$ |  no. of movies rated by user j |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recommenders\n",
    "A recommender system is a system like what you see when you open a streaming service and it shows you a bunch of movies that it thinks you would want to see. In this case it might be based on the ratings other people gave the moves, so it might look like this:   \n",
    "| Movie | Person 1 | person 2 | Person 3 | person 4|  \n",
    "|--|--|--|--|--|\n",
    "| **Love at last**| 5 | 4|5|0|\n",
    "| **Romance forever** | 5| 5| 4| ?|\n",
    "| **Animals in the wild** | ? | 4| ? | 2|\n",
    "| **Cars and cops** | 2 | ? | 3 | 5|\n",
    "| **fighters** | ? | ? | 0 | 4|   \n",
    "  \n",
    "Here $n_u$ = no. of users ($n_4$)  \n",
    "$n_m$ = no. of movies ($m_5$)  \n",
    "$r(i,j)=1$ if user $j$ has rated movie $i$ So r(1,1)=1 and r(2,4)=0  \n",
    "$y^{i,j}$= rating given by user j to movie i (only if $r(i,j)=1 of course)  So $y^{1,1}$=5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding features to your recommender  \n",
    "Let's say you add features to the data you had before:  \n",
    "| Movie | Person 1 | person 2 | Person 3 | person 4| $x_1$ (romance) | $x_2$ (action)|  \n",
    "|--|--|--|--|--|--|--|\n",
    "| **Love at last**| 5 | 4|5|0|0.9 | 0.0|\n",
    "| **Romance forever** | 5| 5| 4| ?| 1.0| 0.01|\n",
    "| **Animals in love** | ? | 4| ? | 2|0.99| 0.0|\n",
    "| **Cars and cops** | 2 | ? | 3 | 5| 0.1| 0.9|\n",
    "| **fighters** | ? | ? | 0 | 4| 0|1|  \n",
    "\n",
    "in this case we would use:   \n",
    "n = number of features (2)  \n",
    "$x^{(1)}$=[$_{0.0}^{0.9}$]  \n",
    "$m^{(j)}$= no. of movies rated by user j \n",
    "  \n",
    "For user 1: prediction rating for any movie $i$ as $w \\cdot x^{(i)}+b$  \n",
    "Or more specific for user 1, if we want to predict movie 3:   \n",
    "if we would choose: $w^{(1)}=$[$_{0}^{5}$] &nbsp; $b^{(1)}=0$ &nbsp; and we know $x^{(3)}=$[$_{0.9}^{0}$]  \n",
    "  \n",
    "We can calculate:  \n",
    "$w^{(1)} \\cdot x^{(3)}+b^{(1)}=4.95$  which is actually quite possible if you look at her other ratings \n",
    "  \n",
    "Generalized that give use for user J:  \n",
    "$w^{(j)} \\cdot x^{(i)}+b{(j)}$   \n",
    "$m^{(j)}$  \n",
    "With the cost function:  \n",
    "$$\\frac{1}{2m^{(j)}} \\sum_{i:r(i,j)=1} (w \\cdot x^{(i)}+b^{(j)}-y^{(i,j)})^2$$  \n",
    "*We use $_{i:r(i,j)=1}$ because people have not rated all movies, so we only sum over the values of i where r(i,j) are equal to 1 (so only the once user j has rated).*  \n",
    "This leads to the full function (including regularization term):  \n",
    "$$^{min}_{w^{(j)}b^{(j)}}J(w^{(j)},b^{(j)}) = \\frac{1}{2m^{(j)}} \\sum_{i:r(i,j)=1} (w \\cdot x^{(i)}+b^{(j)}-y^{(i,j)})^2 + \\frac{\\lambda}{2m^{(j)}}\\sum^{n}_{k=1}(w^{(j)}_k)^2$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. you can take the division by $m^{(j)}$ because its a constant in this function\n",
    "2. if you want to learn the parameters $w^{(1)},b{(1)},w^{(2)},b{(2)},\\cdot \\cdot \\cdot, w^{(n_u)},b{(n_u)}$ for all users:    \n",
    "$$J(\\substack{{w^{(1)},\\cdot \\cdot \\cdot,w^{(n_u)}}\\\\ b{(1)},\\cdot \\cdot \\cdot,b^{(n_u)}})=\\frac{1}{2}\\sum^{n_u}_{j=1} \\sum_{i:r(i,j)=1} (w \\cdot x^{(i)}+b^{(j)}-y^{(i,j)})^2 + \\frac{\\lambda}{2m^{(j)}}\\sum^{n_u}_{j=1}\\sum^{n}_{k=1}(w^{(j)}_k)^2$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collaborative filtering algorithm  \n",
    "#### Finding the values of your features if you don't have them \n",
    "What if you wouldn't have input for your features... in that case you could use, as long as you have data from multiple people, their other data to try and predict it. In that case for an individual user you could use:  \n",
    " $$J(x^{(i)}) = \\frac{1}{2} \\sum_{i:r(i,j)=1} (w \\cdot x^{(i)}+b^{(j)}-y^{(i,j)})^2 + \\frac{\\lambda}{2}\\sum^{n}_{k=1}(w^{(j)}_k)^2$$  \n",
    " Or for all:  \n",
    " $$J(x^{(1)},\\cdot \\cdot \\cdot,x^{(n_m)})=\\frac{1}{2}\\sum^{n_m}_{j=1} \\sum_{i:r(i,j)=1} (w \\cdot x^{(i)}+b^{(j)}-y^{(i,j)})^2 + \\frac{\\lambda}{2}\\sum^{n_m}_{i=1}\\sum^{n}_{k=1}(x^{(i)}_k)^2$$  \n",
    "   \n",
    "Now you can combine all of these to find missing values for all 3 the options by using this:  \n",
    "$$J(\\substack{w^{(1)},\\cdot \\cdot \\cdot,w^{(n_u)}\\\\ b{(1)},\\cdot \\cdot \\cdot,b^{(n_u)}\\\\ x{(1)},\\cdot \\cdot \\cdot,x^{(n_m)}})=\\frac{1}{2}\\sum_{(i,j):r(i,j)=1} (w{(j)} \\cdot x^{(i)}+b^{(j)}-y^{(i,j)})^2 + \\frac{\\lambda}{2}\\sum^{n_u}_{j=1}\\sum^{n}_{k=1}(w^{(j)}_k)^2+\\frac{\\lambda}{2}\\sum^{n_m}_{i=1}\\sum^{n}_{k=1}(x^{(i)}_k)^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What would be the best cost function?\n",
    "You could use gradient decent:  \n",
    "Instead of:  \n",
    "$$w_i=w_i-\\alpha\\frac{\\partial}{\\partial w_i}J(w,b)$$\n",
    "$$b=b-\\alpha\\frac{\\partial}{\\partial b}J(w,b)$$  \n",
    "  \n",
    "You would simultaneously update:  \n",
    "$$w_i^{(j)}=w_i^{(j)}-\\alpha\\frac{\\partial}{\\partial w_i^{(j)}}J(w,b,x)$$\n",
    "$$b^{(j)}=b^{(j)}-\\alpha\\frac{\\partial}{\\partial b^{(j)}}J(w,b)$$  \n",
    "$$x_k^{(i)}=x_k^{(i)}-\\alpha\\frac{\\partial}{\\partial x_k^{(i)}}J(w,b,x)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How to work with binary labels  \n",
    "In a lot of these predicter cases you will get either a 1,0 or a ?. 1 could mean that the user has either liked it or most common spent enough time looking at it that it's relevant. 0 means that they disliked it or that they didn't spend enough time on int and ? means no data.  \n",
    " \n",
    "To work with this you need to the be able to predict y, like this:  \n",
    "$$g(z)=\\frac{1}{1+e^{-z}}$$   \n",
    "And the cost function to:  \n",
    "$$j(w,b,x)=\\sum_{(i,j):r(i,j)=1}L(f_{(w,b,x)})(x),y^{(i,j)})$$  \n",
    "  \n",
    "  \n",
    "*where $f_{(w,b,x)}(x) == g(z)=\\frac{1}{1+e^{-z}}$*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mean normalization  \n",
    "Mean normalization is an important step to make you algorithm run smoother. One of the things that likely happen is the case of now prior info, is that $w$ and $b$ will be 0. Which will result that for the formula:  \n",
    "$$J(\\substack{w^{(1)},\\cdot \\cdot \\cdot,w^{(n_u)}\\\\ b{(1)},\\cdot \\cdot \\cdot,b^{(n_u)}\\\\ x{(1)},\\cdot \\cdot \\cdot,x^{(n_m)}})=\\frac{1}{2}\\sum_{(i,j):r(i,j)=1} (w{(j)} \\cdot x^{(i)}+b^{(j)}-y^{(i,j)})^2 + \\frac{\\lambda}{2}\\sum^{n_u}_{j=1}\\sum^{n}_{k=1}(w^{(j)}_k)^2+\\frac{\\lambda}{2}\\sum^{n_m}_{i=1}\\sum^{n}_{k=1}(x^{(i)}_k)^2$$  \n",
    "every outcome will be 0. Which is not helpful. \n",
    "  \n",
    "To prevent that from happening you do a mean normalization. That means that if you have the following rating of the other users and ? for the unknowns:  \n",
    "5 5 0 0 ?  \n",
    "5 ? ? 0 ?  \n",
    "? 4 0 ? ?  \n",
    "0 0 5 4 ?\n",
    "0 0 5 0 ?  \n",
    "  \n",
    "You can calculate the average ($\\mu$)  \n",
    "2.5  \n",
    "2.5  \n",
    "2  \n",
    "2.25  \n",
    "1.25  \n",
    "\n",
    "The last step is to subtract the average from all the users. This gives your the mean normalization and will allow you to calculate a starting score for a new user.  \n",
    "  \n",
    "In this case we took the means of the rows (movie ratings), so  that we can add a new column (users). You can do the opposite too. This would allow you to give a suggested rating for a new movie. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensorflow  \n",
    "Why do all the calculations if you can use Tensorflow to do it for you. In this case we need to make some manual entries, because simply using the dense function doesn't exactly work. Tensorflow can calculate the cost function using:  \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def cofi_cost_func_v(X, W, b, Y, R, lambda_):\n",
    "    \"\"\"\n",
    "    Returns the cost for the content-based filtering\n",
    "    Vectorized for speed. Uses tensorflow operations to be compatible with custom training loop.\n",
    "    Args:\n",
    "      X (ndarray (num_movies,num_features)): matrix of item features\n",
    "      W (ndarray (num_users,num_features)) : matrix of user parameters\n",
    "      b (ndarray (1, num_users)            : vector of user parameters\n",
    "      Y (ndarray (num_movies,num_users)    : matrix of user ratings of movies\n",
    "      R (ndarray (num_movies,num_users)    : matrix, where R(i, j) = 1 if the i-th movies was rated by the j-th user\n",
    "      lambda_ (float): regularization parameter\n",
    "    Returns:\n",
    "      J (float) : Cost\n",
    "    \"\"\"\n",
    "    j = (tf.linalg.matmul(X, tf.transpose(W)) + b - Y)*R\n",
    "    J = 0.5 * tf.reduce_sum(j**2) + (lambda_/2) * (tf.reduce_sum(X**2) + tf.reduce_sum(W**2))\n",
    "    return J"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mUntitled-1.ipynb Cell 11\u001b[0m in \u001b[0;36m<cell line: 13>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:Untitled-1.ipynb?jupyter-notebook#X14sdW50aXRsZWQ%3D?line=11'>12</a>\u001b[0m iterations \u001b[39m=\u001b[39m \u001b[39m200\u001b[39m\n\u001b[0;32m     <a href='vscode-notebook-cell:Untitled-1.ipynb?jupyter-notebook#X14sdW50aXRsZWQ%3D?line=12'>13</a>\u001b[0m \u001b[39mfor\u001b[39;00m \u001b[39miter\u001b[39m \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(iterations):\n\u001b[0;32m     <a href='vscode-notebook-cell:Untitled-1.ipynb?jupyter-notebook#X14sdW50aXRsZWQ%3D?line=13'>14</a>\u001b[0m     \u001b[39m# Compute the cost (forward pass included in cost)\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:Untitled-1.ipynb?jupyter-notebook#X14sdW50aXRsZWQ%3D?line=14'>15</a>\u001b[0m     cost_value \u001b[39m=\u001b[39m cofi_cost_func_v(X, W, b, Ynorm, R, lambda_)\n\u001b[0;32m     <a href='vscode-notebook-cell:Untitled-1.ipynb?jupyter-notebook#X14sdW50aXRsZWQ%3D?line=16'>17</a>\u001b[0m     \u001b[39m# Use the gradient tape to automatically retrieve\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:Untitled-1.ipynb?jupyter-notebook#X14sdW50aXRsZWQ%3D?line=17'>18</a>\u001b[0m     \u001b[39m# the gradients of the trainable variables with respect to the loss\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:Untitled-1.ipynb?jupyter-notebook#X14sdW50aXRsZWQ%3D?line=18'>19</a>\u001b[0m     grads \u001b[39m=\u001b[39m tape\u001b[39m.\u001b[39mgradient( cost_value, [X,W,b] )\n",
      "\u001b[1;31mNameError\u001b[0m: name 'X' is not defined"
     ]
    }
   ],
   "source": [
    "from tensorflow import keras\n",
    "\n",
    "w = tf.Variable(3.0)\n",
    "x = 1.0\n",
    "y=1.0 # target value\n",
    "alpha = 0.01 \n",
    "lambda_ = 1\n",
    "\n",
    "\n",
    "#using adam optimizer\n",
    "optimizer = keras.optimizers.Adam(learning_rate=1e-1)\n",
    "iterations = 200\n",
    "for iter in range(iterations):\n",
    "    # Compute the cost (forward pass included in cost)\n",
    "    cost_value = cofi_cost_func_v(X, W, b, Ynorm, R, lambda_)\n",
    "\n",
    "    # Use the gradient tape to automatically retrieve\n",
    "    # the gradients of the trainable variables with respect to the loss\n",
    "    grads = tape.gradient( cost_value, [X,W,b] )\n",
    "\n",
    "    # Run one step of gradient descent by updating\n",
    "    # the value of the variables to minimize the loss.\n",
    "    optimizer.apply_gradients( zip(grads, [X,W,b]) )\n",
    "\n",
    "    # Log periodically.\n",
    "    if iter % 20 == 0:\n",
    "        print(f\"Training loss at iteration {iter}: {cost_value:0.1f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a prediction using trained weights and biases\n",
    "p = np.matmul(X.numpy(), np.transpose(W.numpy())) + b.numpy()\n",
    "\n",
    "#restore the mean\n",
    "pm = p + Ymean\n",
    "\n",
    "my_predictions = pm[:,0]\n",
    "\n",
    "# sort predictions\n",
    "ix = tf.argsort(my_predictions, direction='DESCENDING')\n",
    "\n",
    "for i in range(17):\n",
    "    j = ix[i]\n",
    "    if j not in my_rated:\n",
    "        print(f'Predicting rating {my_predictions[j]:0.2f} for movie {movieList[j]}')\n",
    "\n",
    "print('\\n\\nOriginal vs Predicted ratings:\\n')\n",
    "for i in range(len(my_ratings)):\n",
    "    if my_ratings[i] > 0:\n",
    "        print(f'Original {my_ratings[i]}, Predicted {my_predictions[i]:0.2f} for {movieList[i]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Finding related items\n",
    "Collaborative filtering is also used for this. While it's no really possible to draw a direct line between features $x_1,x_2,...,x_n$ and in the case of movies for example the genre, these features do have information like this. If you want to find movies that are similar, what you are doing is trying to find item $k$ with $x^{(k)}$ similar to $x^{(i)}$ you do this by calculating:  \n",
    "$$\\sum^n_{l=1}(x^{(k)}_l-x^{(i)}_l)^2$$  \n",
    "which can be written as:  \n",
    "$$||x^{(k)}-x^{(i)}||^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Limitations of Collaborative filtering  \n",
    "Collaborative is not good at a \"cold start\" problem:  \n",
    "- How to rank new items that few users have rated?  \n",
    "- How to show something reasonable to new users who have rated few items?  \n",
    "  \n",
    "People deal with this by using side information about users of items"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Content based filtering\n",
    "Another approach is to do content-based filtering. Where collaborative filtering recommends items to you based on rating of users who gave similar ratings as you. Content-based filtering recommends items based on features of user and item to find a good match. So this is more feature based. Where you have for example $x_u$ which is a vector with all the user features and $x_m$ which is a vector with all the movie features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using neural networks  \n",
    "Since you want to use to sets of features, you also need 2 neural networks. Where we calculate $V_u$ for $x_u$ and $V_m$ for $x_m$. Once you have both, you can take the dot product $V_u \\cdot V_m$. In both cases a lot can differ between the neural networks, but the output layer needs to have the same amount of features. To make the algorithm work better, it is good to normalize the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Making the process more efficiently\n",
    "Especially if you have big data to train your model on, it might be computational not feasible. Instead people run it in 2 steps. The Retrieval & Ranking steps.  \n",
    "**Retrieval**  \n",
    "So to stick with the movie example. To generate a list with suggestions. One my first friend 10 movies that are most similar to the last 10 movies the user watched. \n",
    "MMaybe you would want some variations so you could look for the top 3 most viewed genres and add the 10top 10 movies of those and add the top 10 movies of the users country.  \n",
    "\n",
    "These lists you combine, and after removing duplicates and already watched.   \n",
    "**Ranking**  \n",
    "You use your new list, and feed it to the neural network. Based on this you will have a ranked list for the specific user.  \n",
    "  \n",
    "This is quick because you limit the numbers by a lot. You still want to figure out how many you want to show. You should test this before, when it becomes too slow to retrieve too much vs when you have too few to show a good option. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ba80823ec9edfa2b915b0e8b60f600e1ca4cb90dcbbf7f0d0e0246605aecf2a6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
