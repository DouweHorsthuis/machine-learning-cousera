{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Specialization\n",
    "## Unsupervised Learning, Recommenders, Reinforcement Learning\n",
    "# Week 10  \n",
    "  \n",
    "## Terminology\n",
    "\n",
    "| Definition        | Explanation            |\n",
    "|-------------------|------------------------|  \n",
    "| State (s) | The state a robot starts in | \n",
    "| Action | the process a robot takes moving out of the original state |  \n",
    "| Reward/Penalty | The way reinforcement learning teaches a robot if they should do something again | \n",
    "| New state (s') | The state the robot is in after a the action it took | \n",
    "| Return | The sum of the reward to go from one state to the final state, weighted by the discount factor |  \n",
    "| discount factor | A way to make the reinforcement learning algorithm a little inpatient so that it prefers less states to get to the end |  \n",
    "| Policy\\controller ($\\pi$) | The name of the function whose job it is to take as input any state s and map it to some action a that it wants us to take. |  \n",
    "|Markov Decision Process (MDP)| The process of reinforcement learning where the decisions are only based on the current state your are in and none of the previous really matter | \n",
    "| State action value function (Q(s,a)) or Q function| \n",
    "It is the return if you start from state ss, take action aa (once), then behave optimally after that.  |  \n",
    "| Q* | Optimal State action value function / Q function | \n",
    "| Bellman equation | $Q(s,a)=R(s)+\\gamma ^{max}_{a'} Q(s',a')$ |  \n",
    "|$\\epsilon - greedy policy$ | A part of the learning algorithm that allows for random choices for a. To prevent it getting stuck in \"bad ideas\" |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement learning  \n",
    "When you want to teach a robotic helicopter to fly by it self, you cannot really rely on machine learning where you would say if $x$ happens do $y$, because it is quite ambiguous what one should do. So having a bunch of training data where we have the outcome as suggested by a pilot is not an ideal way for an algorithm to learn. Instead you need to work with a sort of reward function. You should focus more on what to do rather than h\n",
    "What you can do is every second the helicopter flies well, your reward it with +1. But when it flies poorly you give it a penalty, -1, or if it crashes you give it a big penalty, -1000. The Mars Rover is an example of a robot that uses reinforcement learning. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Mars Rover example\n",
    "Let's say the Mars Rover can be in 6 position. Position 1 has a surface that scientist are very interested in. Position 6 has a surface they are kinda interested in. But all the others are irrelevant. In that case you can assign 100 reward point to position 1, 40 to position 6 and 0 to all the others. However, the rover might figure out that it's closer to surface 6, which can be a reason to go there. It can also choose to go first to the right only to decide later to abort and go to the left anyway. For every step it takes, it goes from a state (s) using an action (a) to a reward (R(s)) and ends in a new state (s') (s,a,R(s),s'). \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Discount factor\n",
    "When your robot has to choose to go through a lot of states to get to the 100 reward, the **Discount factor** will make it lean towards wanting to go the other way. The discount factor, is a weighted number that decreases the rewards for each extra state your robot has to go through, so that it becomes sometimes better to take the shorter route.   \n",
    "If we continue the example from above and set the discount factor to 0.5. And we start in state 3 and move to the left. We would calculate:  \n",
    "$Return=0+(0.5)0+(0.5)^2 0+(0.5)^3 100= 12.5$  \n",
    "If we would go from state 4 to the right it would be:  \n",
    "$Return= 0+(0.5)0+(0.5)^2 40=10$\n",
    "| |$s^1$|$s^2$|$s^3$|$s^4$|$s^5$|$s^6$|  \n",
    "|--|--|--|--|--|--|--|\n",
    "|Reward per state|100|0|0|0|0|40|\n",
    "|Reward if going all the way left| 100<br> <br>|50<br>&larr;|25<br>&larr;|12.5<br>&larr;|6.25<br>&larr;|40<br> <br>|\n",
    "|Reward if going all the way right| 100<br> <br>|2.5<br> &rarr;|5<br> &rarr;|10<br> &rarr;|20<br> &rarr;|40<br> <br>|  \n",
    "|Likely decicion for a robot | | &larr; | &larr; | &larr; | &rarr;| |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### State action value function (Q(s,a)) \n",
    "The state action value function (Q(s,a)) is the function of the state you are in an the action you are taking. So if we use the above table and you would say we are in state 2 and decide to go right it would look like Q(2,&rarr;), you would end up in state 3, where the best course of action is to go to the left and you will keep taking the best course of action until you reach the return. This results in:  \n",
    "Q(2,&rarr;)=$0+(0.5)0+(0.5)^2 0+(0.5)^3 100=12.5$.  \n",
    "If you would go left:  \n",
    "Q(2,&Larr;)=$0+(0.5)100=50$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bellman Equation  \n",
    "The Bellman equation is the most important equation for reinforcement learning. We will use the following notations:  \n",
    "s : current state  \n",
    "a : current action  \n",
    "s': state you get to after taking action a  \n",
    "a': action that you take in state s'  \n",
    "  \n",
    "The bellman equation= $Q(s,a)=R(s)+\\gamma ^{max}_{a'} Q(s',a')$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Continues states\n",
    "In all the previous examples, we are talking about state like it is limited amount of options. But in general there are way more possibilities. Besides length being not 1-6 but maybe 1-6km where 1.54km is a valid option. But also there are multiple variables (left right up down speed etc.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How to train a neural network? \n",
    "Since you might not have any data points. You can just try and have your learning algorithm takes random choices. Like this you might get 10000 data points. In that case X=(s,a) and $y=R(s)+\\gamma ^{max}_{a'}Q(s',a')$ So by running it like this, you will have a better guess to start off with, you would call this $Q_{new}$. If you set Q=$Q_{new}$. You can repeat that and get a better algorithm every time. If you worry that $Q_{new}$ is not great, you can also decide to use a Soft Update. Where you Set Q=0.01$Q_{new}$+0.99$Q$. The best way of getting the most efficient neural network, you should have the amount of units in the output layer that is equal as the amount of Qs you want. To prevent that the algorithm gets stuck in a \"bad idea\". For example, the algorithm might by chance learns that something should never happen (which might be true at the start). To prevent it getting stuck its good to for a probability of 0.95 always try to maximize Q(s,a), but for 0.05 do it randomly. This is called a $\\epsilon - greedy policy$ You can also start high and gradually decrease, so that a \"young\" algorithm has a lot of learning from random choices but a more mature one should rely more on it's prior knowledge. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ba80823ec9edfa2b915b0e8b60f600e1ca4cb90dcbbf7f0d0e0246605aecf2a6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
