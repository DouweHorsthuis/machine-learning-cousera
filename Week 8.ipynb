{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Specialization\n",
    "## Unsupervised Learning, Recommenders, Reinforcement Learning\n",
    "# Week 8  \n",
    "  \n",
    "## Terminology\n",
    "\n",
    "| Definition        | Explanation            |\n",
    "|-------------------|------------------------|  \n",
    "| Clustering algorithm | an algorithm that looks at a number of data points and automatically finds data points that are related or similar |  \n",
    "| Cluster centroids | The center (or suspected center) of a bunch of data points |  \n",
    "| $\\mu_n$ | The vectors whose average is the location of a centroid |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering  \n",
    "When comparing supervised learning with unsupervised learning, the main difference is that for the first you you have **an X and a Y input**  for your training. For unsupervised learning you only have a X feature.  \n",
    "The unsupervised clustering algorithm will try to cluster datasets within the whole group together. Without knowing what the is the \"right\" answer or Y. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-means clustering algorithm  \n",
    "A K-means clustering algorithm will randomly pick 2 points where might be the center of 2 clusters. After that it cycle through all the data points and see if it's closer to centroid one or centroid two. The second step is that it then takes the average of all the data points that are associated with centroid one and centroid 2 and move the centroids to their average location. After that it will re-do the same thing, and some data point will swap now because they might be closer to the other centroid now. The more iterations you run the closer you get to the actual center of 2  groups. You would know you reached the center, because the average will not or only hardly change after another iteration. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### K-means algorithm  \n",
    "First you need to randomly initialize $K$ cluster centroids $\\mu_1,\\mu_2,\\cdots,\\mu_K$. This corresponds to the random picking of the starting centers. To clarify, $\\mu_1,\\mu_2,\\cdots,\\mu_K$ are vectors that have the same dimensions as your training samples. $K$ stands for the amount of clusters you want to find.   \n",
    "After that you want to calculate the distance between $\\mu_k$ and your data points. In math:  \n",
    "$min_k || x^{(i)}-\\mu_k ||$ This is calculating the smallest distance between X(datapoint) and $\\mu$ (centroid)  \n",
    "A different notation would be:  \n",
    "Repeat{  \n",
    "    # Assign pints to cluster centroids  \n",
    "    for $i$=1 to $m$  \n",
    "    $c^{(i)}$:=index(from 1 to $K$) of cluster  \n",
    "    Centroid to closest to $x^{(i)}$  \n",
    "    # Move cluster centroids\n",
    "    for $k$ =1 to $K$  \n",
    "        $\\mu_k$:= average (mean) of points assigned to cluster $k$\n",
    "}  \n",
    "  \n",
    "$c^{(i)}$ is the index of a cluster (1,2,...,$K$) to which example $x^{(i)}$ is currently assigned.  \n",
    "$\\mu_k$ = cluster centroid $k$  \n",
    "$\\mu_{c^{(i)}}$=cluster centroid of a cluster to which example $x^{(i)}$ has been assigned. (example: to which centroid was training example 10 ($x^{(10)} $) assigned? You look for $c^{(10)}$. Then $\\mu_{c^{(10)}}$ would be the location of the centroid to which training example 10 was assigned).\n",
    "\n",
    "**Note 1**, *what to do if a cluster gets 0 data points assigned*: there are 2 options, you either continue with k=k-1 clusters (so you delete that centroid) or you re-initialize it from the start, hoping that the next random first choice would give all your clusters at least a data point (the first one is most common).  \n",
    "  \n",
    "**note 2**, *you can use a clustering algorithm on data that doesn't have very distinct clusters*. It's not uncommon to use a k-means cluster on data that seems continues or somewhat random.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cost function for K-means or distortion (cost) function\n",
    "$$J(c^{(1)},...,c^{(m)},\\mu_1,...,\\mu_K)=\\frac{1}{m}\\sum^m_{i=1}||x^{(i)}-\\mu_{c^{i}}||^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initializing K-means\n",
    "As mentioned before, the first locations of the centroids are random. Since one should always have $K<m$ centroids (it wouldn't make sense to look for more cluster than that there are data points), one should pick a random dataset point to start.  \n",
    "However, it is true that the starting point will impact which clusters will be found. Because of that, it's good to run it multiple times. After that you can run the cost function on all the initializations and see what gives you the lowest number. This translated to:  \n",
    "  \n",
    "\n",
    "For i = 1 to 100 {  \n",
    "    Randomly initialize K-means.  \n",
    "    Run K-means. Get $c^{(1)},...,c^{(m)},\\mu_1,...,\\mu_K$  \n",
    "    Compute cost function (distortion)  \n",
    "    $J(c^{(1)},...,c^{(m)},\\mu_1,...,\\mu_K)$  \n",
    "}  \n",
    "Pick set of clusters that gave lowest cost $J$  \n",
    "  \n",
    "It's pretty common to run this 50-1000 times "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How many clusters will you use?  \n",
    "The right value of $K$ is usually ambiguous. Just because we see 2 groups, it doesn't perse mean that there aren't more. One method one can use is the **Elbow method**.  \n",
    "For this method you plot *Cost function $J$* vs the *$K$ (no. of clusters) and look where the cost doesn't really decrease that much anymore. One of the downsides is that you often won't find a real elbow. Instead you'll find a smooth decrease and thus making it impossible to choose.  \n",
    "Another way of doing it (better) is to think of **how many clusters you want for a later purpose**. For example, if you have height and weight of your average t-shirt buyers, and you want to decide how big your shirts should be. It might make most sense to go for 3 clusters so you end up with a small, medium and large. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Anomaly detection  \n",
    "Another unsupervised learning technic is anomaly detection. Where you search for anomalies in the data. What happens here is that you have test datasets:{$x^{(1)},x^{(2)},...,x^{(m)}$ and you want to know if $x_{test}$ is anomalous. You can do this by building a model that give you the probability of x (Model p(x)).  \n",
    "![probability model](https://github.com/DouweHorsthuis/machine-learning-cousera/blob/main/images/probabilty.png?raw=true)  \n",
    "*Here the p to be in the center would be high, the p for the second circle would be lower and the p for the third even lower.*  \n",
    "You would use $p(x_{test})< \\epsilon $ -> where if this is true it would flag as a anomaly. $\\epsilon$ would be a small number here.  \n",
    "  \n",
    "Anomaly detection is often used in fraud detection and manufacturing. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gaussian (normal) distribution  \n",
    "A different way of thinking is to use a gaussian distribution, or a normal distribution. You can calculate a gaussian distribution for your data by using:  \n",
    "$p(x)=\\frac{1}{\\sqrt{2\\pi}\\sigma}e^{\\frac{-(x-\\mu)^2}{2\\sigma^2}}$  \n",
    "Here the width is determined by $\\sigma$ and where $\\mu$ sets the center. \n",
    "If the data falls on the outer edge, you get a very low number (lower than $\\epsilon$). This would mean there is something odd. If you get a high number as you should get most of the time. Your test data is fine."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiple features  \n",
    "Until now we only focused on x having 1 feature. This is very unlikely. While it is better for the features to be statistically independent of each other. It will still work if this is not the case. To estimate $p(x)$ in this case, while x has n features, we work with:  \n",
    "$p(x)=p(x_1)*p(x_2),...,*p(x_n)$  \n",
    "However we also need to find the mean distribution for each of them. So the full formula is:  \n",
    "$$p(x)=p(x_1;\\mu_1,\\sigma^2_1)*p(x_2;\\mu_2,\\sigma^2_2),...,*p(x_n;\\mu_n,\\sigma^2_n) = \\prod^{n}_{j=1} p(x_j;\\mu_j,\\sigma_j^2)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to set it up?  \n",
    "1. First you want to choose $n$ features $x_i$, that you think will show something is anomalous.\n",
    "2. Fit parameter $\\mu_1,...,\\mu_n,\\sigma^2_1,...,\\sigma^2_n$  \n",
    "This allows you to calculate $$\\mu_j=\\frac{1}{m}\\sum^{m}_{i=1}x_j^{(i)}$$  $$\\sigma_j^2=\\frac{1}{m}\\sum^{m}_{i=1}(x_j^{(i)}-\\mu_j)^2$$ or vectorized $$\\overrightarrow{\\mu}=\\frac{1}{m}\\sum^{m}_{i=1}\\overrightarrow{x}^{(i)}$$  \n",
    "3. Given a new example, $x$, compute $p(x)$  \n",
    "$$p(x)=\\prod^{n}_{j=1} p(x_j;\\mu_j,\\sigma_j^2)$$ \n",
    "4. check if $p(x)<\\epsilon$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluations a anomaly detection system\n",
    "To evaluate your model, you will need a couple of labeled anomalies. This means that if you have 10000 datasets having anywhere between 2-50 labeled anomalies is enough to test it. Even if in the 10000 datasets, there are a couple of anomalies that due to not being labeled slip into the \"normal\" data, it should still work. You can do the rest, just like we did before, where you split up your data into for example 6000 training sets, and then you use 2000 normal + 10 anomalies for cross validation and the same amount as your test set.  \n",
    "Because of the lack of labeled data, you could choose to skip the test set part, and only use cross validation. Which works a little less well but might be the best way to go. As long as you are away that you might have a bigger likelihood for overfitting.  \n",
    "It is wort considering to evaluate your metrics using:  \n",
    "- True positive, false positive, false negative, true negative\n",
    "- Precision/Recall\n",
    "- $F_1$-score  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## When to use anomaly detection vs. supervised learning  \n",
    "|Anomaly detection | Supervised learning | \n",
    "|--|--|  \n",
    "|Very small number of positive examples (y=1) (0-20 is common). Larger number of negative (y=0) examples. | Large number of costive and negative examples |  \n",
    "Many \"types\" of anomalies. Hard for any algorithm to learn from positive examples what the anomalies look like; future anomalies may look nothing like any of the anomalous examples seen so far | Enough positive examples for the algorithm to get a sense of what positive examples are like, future positive examples likely to be similar to ones in training set |  \n",
    "|Fraud - this keeps changing| Spam - spam seems to be always somewhat similar|  \n",
    "|manufacturing - Finding new previously unseen defects in manufacturing. (e.g. aircraft engines) | Manufacturing - Finding known, previously seen defects |  \n",
    "Monitoring machines in a data center | Weather prediction (sunny/rainy/etc.) |  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Features for anomaly detection \n",
    "One step is to try to makes sure that your features either are gaussian or make them a bit more gaussian. A good way of checking that is by plotting a histogram of your data. you can do this as `plt.hist` in python. Making it more gaussian could be done by seeing if the log transformation, or $x^{0.n}$ is more gaussian.  As shown in this code below\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from scipy.stats import skewnorm\n",
    "import matplotlib.pyplot as plt  \n",
    "import numpy as np\n",
    "\n",
    "numValues = 10000\n",
    "maxValue = 100\n",
    "skewness = 50 #negative values are left skewed, postive values are right skewed.\n",
    "\n",
    "random= skewnorm.rvs(a=skewness,loc=maxValue, size=numValues) #Skewnorm function\n",
    "\n",
    "random = random -min(random)\n",
    "random = random / max(random) \n",
    "random = random * maxValue\n",
    "\n",
    "x=random\n",
    "\n",
    "plt.subplot(3,1,1);\n",
    "plt.hist(x,bins=50,color='r');\n",
    "plt.subplot(3,1,2);\n",
    "plt.hist(np.log(x+0.001),bins=50, color='g');\n",
    "plt.subplot(3,1,3);\n",
    "plt.hist(x**0.4,bins=50, color='b');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the code above, we see that instead of using which is skewed and now gaussian, using $x^{0.4}$ is more gaussian. So it would be better to use that.  \n",
    "  \n",
    "*Note: because x has 0, taking the log of that would be impossible, so the trick is to add a very small number in that case*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### common probmel in error analysis for anomaly detection  \n",
    "$p(x)$ is comparable for normal and anomalous example. In this case it would be a good idea to spend some time thinking why you think this data point is anomalous. If you are sure, try to find a new feature that you might be able to add to make the algorithm better. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ba80823ec9edfa2b915b0e8b60f600e1ca4cb90dcbbf7f0d0e0246605aecf2a6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
