{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Specialization\n",
    "## Unsupervised Learning, Recommenders, Reinforcement Learning\n",
    "# Week 8  \n",
    "  \n",
    "## Terminology\n",
    "\n",
    "| Definition        | Explanation            |\n",
    "|-------------------|------------------------|  \n",
    "| Clustering algorithm | an algorithm that looks at a number of data points and automatically finds data points that are related or similar |  \n",
    "| Cluster centroids | The center (or suspected center) of a bunch of data points |  \n",
    "| $\\mu_n$ | The vectors whose average is the location of a centroid |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering  \n",
    "When comparing supervised learning with unsupervised learning, the main difference is that for the first you you have **an X and a Y input**  for your training. For unsupervised learning you only have a X feature.  \n",
    "The unsupervised clustering algorithm will try to cluster datasets within the whole group together. Without knowing what the is the \"right\" answer or Y. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-means clustering algorithm  \n",
    "A K-means clustering algorithm will randomly pick 2 points where might be the center of 2 clusters. After that it cycle through all the data points and see if it's closer to centroid one or centroid two. The second step is that it then takes the average of all the data points that are associated with centroid one and centroid 2 and move the centroids to their average location. After that it will re-do the same thing, and some data point will swap now because they might be closer to the other centroid now. The more iterations you run the closer you get to the actual center of 2  groups. You would know you reached the center, because the average will not or only hardly change after another iteration. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### K-means algorithm  \n",
    "First you need to randomly initialize $K$ cluster centroids $\\mu_1,\\mu_2,\\cdots,\\mu_K$. This corresponds to the random picking of the starting centers. To clarify, $\\mu_1,\\mu_2,\\cdots,\\mu_K$ are vectors that have the same dimensions as your training samples. $K$ stands for the amount of clusters you want to find.   \n",
    "After that you want to calculate the distance between $\\mu_k$ and your data points. In math:  \n",
    "$min_k || x^{(i)}-\\mu_k ||$ This is calculating the smallest distance between X(datapoint) and $\\mu$ (centroid)  \n",
    "A different notation would be:  \n",
    "Repeat{  \n",
    "    # Assign pints to cluster centroids  \n",
    "    for $i$=1 to $m$  \n",
    "    $c^{(i)}$:=index(from 1 to $K$) of cluster  \n",
    "    Centroid to closest to $x^{(i)}$  \n",
    "    # Move cluster centroids\n",
    "    for $k$ =1 to $K$  \n",
    "        $\\mu_k$:= average (mean) of points assigned to cluster $k$\n",
    "}  \n",
    "  \n",
    "$c^{(i)}$ is the index of a cluster (1,2,...,$K$) to which example $x^{(i)}$ is currently assigned.  \n",
    "$\\mu_k$ = cluster centroid $k$  \n",
    "$\\mu_{c^{(i)}}$=cluster centroid of a cluster to which example $x^{(i)}$ has been assigned. (example: to which centroid was training example 10 ($x^{(10)} $) assigned? You look for $c^{(10)}$. Then $\\mu_{c^{(10)}}$ would be the location of the centroid to which training example 10 was assigned).\n",
    "\n",
    "**Note 1**, *what to do if a cluster gets 0 data points assigned*: there are 2 options, you either continue with k=k-1 clusters (so you delete that centroid) or you re-initialize it from the start, hoping that the next random first choice would give all your clusters at least a data point (the first one is most common).  \n",
    "  \n",
    "**note 2**, *you can use a clustering algorithm on data that doesn't have very distinct clusters*. It's not uncommon to use a k-means cluster on data that seems continues or somewhat random.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cost function for K-means or distortion (cost) function\n",
    "$$J(c^{(1)},...,c^{(m)},\\mu_1,...,\\mu_K)=\\frac{1}{m}\\sum^m_{i=1}||x^{(i)}-\\mu_{c^{i}}||^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initializing K-means\n",
    "As mentioned before, the first locations of the centroids are random. Since one should always have $K<m$ centroids (it wouldn't make sense to look for more cluster than that there are data points), one should pick a random dataset point to start.  \n",
    "However, it is true that the starting point will impact which clusters will be found. Because of that, it's good to run it multiple times. After that you can run the cost function on all the initializations and see what gives you the lowest number. This translated to:  \n",
    "  \n",
    "\n",
    "For i = 1 to 100 {  \n",
    "    Randomly initialize K-means.  \n",
    "    Run K-means. Get $c^{(1)},...,c^{(m)},\\mu_1,...,\\mu_K$  \n",
    "    Compute cost function (distortion)  \n",
    "    $J(c^{(1)},...,c^{(m)},\\mu_1,...,\\mu_K)$  \n",
    "}  \n",
    "Pick set of clusters that gave lowest cost $J$  \n",
    "  \n",
    "It's pretty common to run this 50-1000 times "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How many clusters will you use?  \n",
    "The right value of $K$ is usually ambiguous. Just because we see 2 groups, it doesn't perse mean that there aren't more. One method one can use is the **Elbow method**.  \n",
    "For this method you plot *Cost function $J$* vs the *$K$ (no. of clusters) and look where the cost doesn't really decrease that much anymore. One of the downsides is that you often won't find a real elbow. Instead you'll find a smooth decrease and thus making it impossible to choose.  \n",
    "Another way of doing it (better) is to think of **how many clusters you want for a later purpose**. For example, if you have height and weight of your average t-shirt buyers, and you want to decide how big your shirts should be. It might make most sense to go for 3 clusters so you end up with a small, medium and large. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Anomaly detection  \n",
    "Another unsupervised learning technic is anomaly detection. Where you search for anomalies in the data. What happens here is that you have test datasets:{$x^{(1)},x^{(2)},...,x^{(m)}$ and you want to know if $x_{test}$ is anomalous. You can do this by building a model that give you the probability of x (Model p(x)).  \n",
    "![probability model](https://github.com/DouweHorsthuis/machine-learning-cousera/blob/main/images/probabilty.PNG?raw=true)  \n",
    "*Here the p to be in the center would be high, the p for the second circle would be lower and the p for the third even lower.*  \n",
    "You would use $p(x_{test})< \\epsilon $ -> where if this is true it would flag as a anomaly. $\\epsilon$ would be a small number here.  \n",
    "  \n",
    "Anomaly detection is often used in fraud detection and manufacturing. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gaussian (normal) distribution  \n",
    "A different way of thinking is to use a gaussian distribution, or a normal distribution. You can calculate a gaussian distribution for your data by using:  \n",
    "$p(x)=\\frac{1}{\\sqrt{2\\pi}\\sigma}e^{\\frac{-(x-\\mu)^2}{2\\sigma^2}}$  \n",
    "Here the width is determined by $\\sigma$ and where $\\mu$ sets the center. \n",
    "If the data falls on the outer edge, you get a very low number (lower than $\\epsilon$). This would mean there is something odd. If you get a high number as you should get most of the time. Your test data is fine."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiple features  \n",
    "Until now we only focused on x having 1 feature. This is very unlikely. While it is better for the features to be statistically independent of each other. It will still work if this is not the case. To estimate $p(x)$ in this case, while x has n features, we work with:  \n",
    "$p(x)=p(x_1)*p(x_2),...,*p(x_n)$  \n",
    "However we also need to find the mean distribution for each of them. So the full formula is:  \n",
    "$$p(x)=p(x_1;\\mu_1,\\sigma^2_1)*p(x_2;\\mu_2,\\sigma^2_2),...,*p(x_n;\\mu_n,\\sigma^2_n) = \\prod^{n}_{j=1} p(x_j;\\mu_j,\\sigma_j^2)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to set it up?  \n",
    "1. First you want to choose $n$ features $x_i$, that you think will show something is anomalous.\n",
    "2. Fit parameter $\\mu_1,...,\\mu_n,\\sigma^2_1,...,\\sigma^2_n$  \n",
    "This allows you to calculate $$\\mu_j=\\frac{1}{m}\\sum^{m}_{i=1}x_j^{(i)}$$  $$\\sigma_j^2=\\frac{1}{m}\\sum^{m}_{i=1}(x_j^{(i)}-\\mu_j)^2$$ or vectorized $$\\overrightarrow{\\mu}=\\frac{1}{m}\\sum^{m}_{i=1}\\overrightarrow{x}^{(i)}$$  \n",
    "3. Given a new example, $x$, compute $p(x)$  \n",
    "$$p(x)=\\prod^{n}_{j=1} p(x_j;\\mu_j,\\sigma_j^2)$$ \n",
    "4. check if $p(x)<\\epsilon$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ba80823ec9edfa2b915b0e8b60f600e1ca4cb90dcbbf7f0d0e0246605aecf2a6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
