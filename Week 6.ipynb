{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Specialization\n",
    "## Advanced learning Algorithms\n",
    "# Week 6  \n",
    "  \n",
    "## Terminology\n",
    "\n",
    "| Definition        | Explanation            |\n",
    "|-------------------|------------------------|\n",
    "| Diagnostics | A test you can run to gain insight into what is/isn't working |  \n",
    "| training data | data used to train your model |\n",
    "| test data | data to test if the model works| \n",
    "| cross validation data / validation set / development set / dev set | data to cross validate if the model works |  \n",
    "|$m_{train}$ | You can use this to emphasize that you are using training data|   \n",
    "|$m_{test}$ | You can use this to emphasize that you are using test data|  \n",
    "|$m_{cv}$ | You can use this to emphasize that you are using cross validation data| \n",
    "|generalization error| a number that tells you how big the error is for real data | \n",
    "| High bias | a model that underfits |\n",
    "| High varience | a model that overfits |  \n",
    "| Baseline level of performance | what is the level of error you can reasonably hope to get |\n",
    "|synthetic data| Data you create from scratch that isn't real world data | \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to run diagnostics\n",
    "## Debugging a learning algorithm\n",
    "You could take the following steps to debug a linear regression that give unacceptably large errors in prediction:  \n",
    "1. Get more training examples\n",
    "2. Try smaller sets of features\n",
    "3. Try getting additional features\n",
    "4. Try adding polynomial features ($x_1^2,x_2^2,x_1,x_2, $ etc)  \n",
    "5. Try decreasing $\\lambda$\n",
    "6. Try increasing $\\lambda$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, it's important to figure out what would be the best point to change for your data. For example you could spend weeks getting more data, but what if that isn't what is needed? Because of that we need to be able to run diagnostics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to evaluate a model\n",
    "\n",
    "### Testing for linear regression (with squared error cost)  \n",
    "When we think of a model that is overly complex like this one:  \n",
    "![](https://github.com/DouweHorsthuis/machine-learning-cousera/blob/main/images/overfit.PNG?raw=true)  \n",
    "We know that $f_{\\overrightarrow{w}b}(\\overrightarrow{x})=w_1x+w_2x^2+w_3x^3+w_4x^4 +b$ is a great fit for the training data, but it's highly unlikely to work for new data, and thus will give poor predictions. But if you want to you can plot this, and see. The problem is, when you have more features, because if you add 3 features, how do you plot a 4 dimensional plot? And if you can, how do you see easily if the new data fits well.  \n",
    "A way to deal with this is to split your training data into 2 subsets (for example 70%/30% or 80%/20%). Where you use 70% of the data as your training set. You can use the remaining 30% to test the model.  \n",
    "You can first **Fit the parameter by minimizing cost function**  $$J(\\overrightarrow{w},b)=^{min}_{\\overrightarrow{w},b} \\left[ \\frac{1}{2m_{train}} \\sum_{i=1}^{m_{train}} (f\\overrightarrow{w},b(\\overrightarrow{x}^{(i)})-y^{(i)})^2+\\frac{\\lambda}{2m_{train}}\\sum_{j=1}^{n} w^2_j\\right]$$   \n",
    "After that you **compute training error**  \n",
    "$$J_{train}(\\overrightarrow{w},b)=\\frac{1}{2m_{train}}\\left[ \\sum_{i=1}^{m_{train}}(f\\overrightarrow{w},b(\\overrightarrow{x}^{(i)}_{train})-y^{(i)}_{train})^2\\right]$$  \n",
    "Then you **compute test error**  \n",
    "$$J_{test}(\\overrightarrow{w},b)=\\frac{1}{2m_{test}}\\left[ \\sum_{i=1}^{m_{test}}(f\\overrightarrow{w},b(\\overrightarrow{x}^{(i)}_{test})-y^{(i)}_{test})^2\\right]$$  \n",
    "  \n",
    "In the example above, you should get a very low cost for the training data, since the data fits perfectly, however the test data should give you a high number since the cost will be high. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing for classification problem  \n",
    "In essence you could do the same as for linear regression. But of course you do it using the cost function for a classification problem. This means **Fit the parameter by minimizing cost function**  $$J(\\overrightarrow{w},b)=-\\frac{1}{m} \\sum_{i=1}^{m} \\left[y^{(i)}log\\left(f\\overrightarrow{w},b(\\overrightarrow{x}^{(i)})\\right)+(1-y^{(i)})log\\left(1-f\\overrightarrow{w},b(\\overrightarrow{x}^{(i)})\\right) \\right] +\\frac{\\lambda}{2m}\\sum_{j=1}^nw^2_j$$  \n",
    "After that you **compute training error**  \n",
    "$$J_{train}(\\overrightarrow{w},b)=-\\frac{1}{m_{train}} \\sum_{i=1}^{m_{train}} \\left[y^{(i)}_{train}log\\left(f\\overrightarrow{w},b(\\overrightarrow{x}^{(i)}_{train})\\right)+(1-y^{(i)}_{train})log\\left(1-f\\overrightarrow{w},b(\\overrightarrow{x}^{(i)}_{train})\\right) \\right] $$  \n",
    "Then you **compute test error**  \n",
    "$$J_{test}(\\overrightarrow{w},b)=-\\frac{1}{m_{test}} \\sum_{i=1}^{m_{test}} \\left[y^{(i)}_{test}log\\left(f\\overrightarrow{w},b(\\overrightarrow{x}^{(i)}_{test})\\right)+(1-y^{(i)}_{test})log\\left(1-f\\overrightarrow{w},b(\\overrightarrow{x}^{(i)}_{test})\\right) \\right] $$  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, a more common approach for this is to calculate the fraction of the test set and the fraction of the train set that the algorithm has misclassified. Which you can do by calculating how many miscalculations have been made for the training and for the test set. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding your best model \n",
    "We can now use the test error to figure out what is the best model to use. For example if you have the following models, but you are not sure which one is best:  \n",
    "d=1 &nbsp; $f_{\\overrightarrow{w},b}(\\overrightarrow{x})=w_1x_1+b$  \n",
    "d=2 &nbsp; $f_{\\overrightarrow{w},b}(\\overrightarrow{x})=w_1x_1+w_2x^2+b$  \n",
    "d=3 &nbsp; $f_{\\overrightarrow{w},b}(\\overrightarrow{x})=w_1x_1+w_2x^2+w_3x^3+b$  \n",
    "&nbsp; $\\vdots$&nbsp; &nbsp; &nbsp; &nbsp; &nbsp;$\\vdots$  \n",
    "d=10 &nbsp;  $f_{\\overrightarrow{w},b}(\\overrightarrow{x})=w_1x_1+w_2x^2+w_3x^3+\\dots w_{10}x^{10}+b$  \n",
    "What you can do is compute the test error for each of them, and use the one that has the lowest test error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross validation\n",
    "The problem with the before mentioned approach is that if you use the outcome of $$J_{test}$$ to decide what your model will be, you will most likely a too positive outcome. Because you are now saying that if the training data trains the model we want to use this limited data to see how well it tests. Instead of that, you should divide your data not in 2 but 3 groups. Something like:  \n",
    "60% training data ($x^{m_{train}} y^{m_{train}})$  \n",
    "20% cross validation ($x^{m_{cv}} y^{m_{cv}})$  \n",
    "20% test data ($x^{m_{test}} y^{m_{test}})$  \n",
    "  \n",
    "The cross validation error is calculated just like the test and training error, just using the cross validation data:  \n",
    "$$J_{cv}(\\overrightarrow{w},b)=-\\frac{1}{m_{cv}} \\sum_{i=1}^{m_{cv}} \\left[y^{(i)}_{cv}log\\left(f\\overrightarrow{w},b(\\overrightarrow{x}^{(i)}_{cv})\\right)+(1-y^{(i)}_{cv})log\\left(1-f\\overrightarrow{w},b(\\overrightarrow{x}^{(i)}_{cv})\\right) \\right] $$  \n",
    "  \n",
    "**What you should do** is instead of validating using your test set, is validating using your cross validation set. You pick the model that has the lowest error. But you will use your test data, to calculate your generalization error. This makes sure that you didn't accidentally just found a model that works well for your test data.  \n",
    "  \n",
    "**You can do the exact same for neural networks**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bias/Variance  \n",
    "A good way to see if your model is good, is to look at the bias and variance.  \n",
    "![](https://github.com/DouweHorsthuis/machine-learning-cousera/blob/main/images/bias_variance.PNG?raw=true)  \n",
    "While the figure here is pretty clear, for more complex models, its harder to visualize. So in those cases you can mostly rely on this:  \n",
    "**High Bias (underfit)**  \n",
    "$J_{train}$ will be high  \n",
    "$J_{train}\\approx J_{cv}$  \n",
    "  \n",
    "**High variance (overfit)**  \n",
    "$J_{cv}>>J_{train}$  \n",
    "($J_{train}$ might be low)  \n",
    "Human level performance error would be between the training data error and the cross validation error.  \n",
    "*High variance can maybe be solved, or reduced, by more training data*\n",
    "  \n",
    "**High Bias (underfit) & High variance (overfit)**  \n",
    "$J_{train}$ will be high  \n",
    "and $J_{cv}>>J_{train}$  \n",
    "Human level performance error would be significantly lower.  \n",
    "*High bias isn't solved with more training data*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|solution| what does it fix| \n",
    "|--|--|\n",
    "|Get more training examples | high variance|  \n",
    "|Smaller set of features | high variance|  \n",
    "|add more features | high bias |  \n",
    "|Try adding polynomial features ($x^2_2,X^2_2,x_1,X_2, etc.) | high bias |  \n",
    "| Decreasing $\\lambda$ | high bias|  \n",
    "| Increasing $\\lambda$ | high variance |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The impact of regularization on bias and variance  \n",
    "To recap, you use $\\lambda$ for regularization. You do this by assigning a number to $\\lambda$, and add  \n",
    "$$ + \\frac{\\lambda}{2m}\\sum_{j=1}^n w_j^2 \\$$  \n",
    "to the end of your model. Or in code it would look like this:  \n",
    "```python\n",
    "layer = Dense (units=25,activation=\"relu\", kernal_regularizer=L2(0.01))\n",
    "```  \n",
    "Where in `kernal_regularizer=L2(0.01)` 0.01 stands for the number you want to assign to $\\lambda$.  \n",
    "$\\lambda$ will impact how much your features will have inpact on the model. So if you set $\\lambda$=10000, you probably end up with a straight line. Because your formula will basically end up being $f\\overrightarrow{w},b(x)\\approx b$.  \n",
    "In the other extreme, there is no regularization. Which could be an increase chance of overfitting.  \n",
    "**So how do you find the right number for $\\lambda$**  \n",
    "Well, you do the same as mentioned above. You start of with a low $\\lambda$ and calculate the cross validation error. After which you increase $\\lambda$ and do the same:  \n",
    "1\\.&nbsp;  Try $\\lambda$=0.00 -> $J_cv(w^{<1>},b^{<1>})$  \n",
    "2\\.&nbsp;  Try $\\lambda$=0.01 -> $J_cv(w^{<2>},b^{<2>})$  \n",
    "3\\.&nbsp;  Try $\\lambda$=0.02 -> $J_cv(w^{<3>},b^{<3>})$  \n",
    "4\\.&nbsp;  Try $\\lambda$=0.04 -> $J_cv(w^{<4>},b^{<4>})$  \n",
    "5\\. Try $\\lambda$=0.08 -> $J_cv(w^{<5>},b^{<5>})$  \n",
    "&nbsp; &nbsp; &nbsp; $\\vdots$ &nbsp;  &nbsp; &nbsp; &nbsp;  &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;$\\vdots$  \n",
    "12\\. Try $\\lambda$=10 -> $J_cv(w^{<5>},b^{<5>})$  \n",
    "  \n",
    "After that you calculate the generalization error by using the test set\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Establishing a baseline level of performance  \n",
    "To understand if you training bias is high or low, you would need something to compare it to. A way to do this is to compare it to human level performance. Another way is to compare it to a competing algorithm performance. You could also guess base it on your previous experience.  \n",
    "If you have a baseline performance, a training error and a cross validation error. You can compare these to see if you have a problem:  \n",
    "Baseline performance&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; :10.6%  \n",
    "Training error ($J_{train}$)&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; :10.8%  \n",
    "Cross validation error ($J_{cv}$) &nbsp; :14.8%  \n",
    "  \n",
    "You can now say that your training error is low, 10.8-10.6=0.2%  \n",
    "but your cross validation error is high 14.8-10.8=4.0%  \n",
    "So in this problem you have a high variance problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bias and variance in Neural networks\n",
    "Large neural networks are low bias machines when trained on small to medium sized datasets. If you make you neural networks large enough, you can almost always fit your training data well. So here you can apply a different approach. A simple recipe that doesn't always work, but when it does is great:  \n",
    "Does it do well on the training set ($J_{train}(\\overrightarrow{w},b)$) compared to baseline performance?  \n",
    "If not, you have a high bias problem, so increase your hidden layers or amount of units. Keep going through this loop until you are happy with the target level of error.   \n",
    "After that you check, does it do well on the cross validation set? ($J_{cv}(\\overrightarrow{w},b)$).  \n",
    "If no, you have high variance, so you need to add more data.  \n",
    "Start again at the start until you are happy with both variance and bias.\n",
    "![](https://github.com/DouweHorsthuis/machine-learning-cousera/blob/main/images/neural_nw_bias_variance.PNG?raw=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Error Analysis  \n",
    "This comes second after looking for bias and variance for seeing where your algorithm is going wrong. Error analysis refers to looking at the misclassified data and seeing what the mistake is. For example you look for mistakes and see if you can group them in common mistakes. It is worth it to count out how many times you see a problem you can group. Like this you can really see what it is worth it to focus on first. When you have a big data set, lets say 1000 misclassifications, you/your team might not have the time to go through all of them. In that case **randomly** select 100 or a couple 100. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding data, but how?\n",
    "Adding data seems like a solution to creating a better model, practically always. But you might not want to just get more data or all data. It is not a bad idea, but it can be slow and or expensive. Instead if you can **figure out what type of data your model struggle with**, find more of that type so you model can train better on something it has a hard time with.  \n",
    "Another way to go is to **create your own extra training data**. For example, if your model is looking at pictures of letters. You could take one of the pictures of the letter A and increase/rotate/change the color/change the contrast. In this example it is still the letter A. So the model can learn from that. It is key that the \"noise\" you add, is something that could really happen in the real world. You can also do this where you create **synthetic data**. This means creating data from scratch. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The full cycle of a machine learning project\n",
    "1. the scope (what do you want to work on/define project)\n",
    "2. collect data \n",
    "3. train the model (training, error analysis, iterative improvement)\n",
    "4. *optional* back to collecting data and train the model until you are happy\n",
    "5. deploy in production (deploy,monitor and maintain it)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ba80823ec9edfa2b915b0e8b60f600e1ca4cb90dcbbf7f0d0e0246605aecf2a6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
