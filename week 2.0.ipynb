{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Specialization\n",
    "## Supervised Machine Learning: Regression and Classification\n",
    "## Week 2\n",
    "\n",
    "# Terminology\n",
    "\n",
    "| Definition        | Explanation            |\n",
    "|-------------------|------------------------|\n",
    "| *x<sub>j</sub>*| *j<sup>th</sup> feature|\n",
    "| *n* | total number of features|\n",
    "|*$\\overrightarrow{x}$* | A vector or feature of the all training example (the *optional* arrow is to make clear this reference to a vector and not a single number| \n",
    "|*$\\overrightarrow{x}$ <sup>(i)</sup>* | A vector or feature of the *i<sup>th</sup>* training example (the *optional* arrow is to make clear this reference to a vector and not a single number| \n",
    "|*x $_{j}^{(i)}$* | Value of feature *j* in *i<sup>th</sup>* training example|\n",
    "|*$\\overrightarrow{w}$*| Row vector with all the values for *w*|\n",
    "|*b*| single number|\n",
    "|*$\\overrightarrow{w}$* and *b* | parameters of the model|\n",
    "| dot product (e.g. $\\overrightarrow{w}$ $\\cdot$ $\\overrightarrow{x}$ ) | w<sub>1</sub>x<sub>1</sub>+w<sub>2</sub>x<sub>2</sub>+...+w<sub>n</sub>x<sub>n</sub> |  \n",
    "|Learning curve | A curve that shows the divergence of gradient decent (J($\\overrightarrow{w}$,b) on Y-axis # iteration on x-axis)|\n",
    "\n",
    "\n",
    "  \n",
    "# Multiple linear regression \n",
    "Whe you have one feature or variable, the function for a linear regression is *f<sub>w,b</sub> (x)=wx+b*. However, **when using multiple features the formula is like this:**  \n",
    "*f<sub>w,b</sub> (x)=w<sub>1</sub>x<sub>1</sub>+w<sub>2</sub>x<sub>2</sub>+...+w<sub>n</sub>x<sub>n</sub>+b*. A simpler way of writing it, implies using a dot product:  \n",
    "*$f$ <sub>$\\overrightarrow{w}$,b</sub>($\\overrightarrow{x}$)=$\\overrightarrow{w}$ $\\cdot$ $\\overrightarrow{x}$ + b*\n",
    "\n",
    "# Vectorization\n",
    "Without vectorization the formula would be ike this:  \n",
    "*f<sub>w,b</sub> (x)=w<sub>1</sub>x<sub>1</sub>+w<sub>2</sub>x<sub>2</sub>+w<sub>3</sub>x<sub>3</sub>+b* which would be very slow to write and to compute. Without vectorization you can increase the speed by:  \n",
    "*$f$ <sub>$\\overrightarrow{w}$,b</sub>($\\overrightarrow{x}$)= $\\sum\\limits_{j=1}^{n}$ w<sub>j</sub>x<sub>j</sub> + b*.  \n",
    "  \n",
    "![why vectorization is quicker](https://github.com/DouweHorsthuis/machine-learning-cousera/blob/main/images/vectorization.PNG?raw=true)  \n",
    "This figure shows that vectorization is quicker because everything is calculated in parallel and the last step is done using specialized hardware.    \n",
    "  \n",
    "![Overview of vector notation vs previous notation](https://github.com/DouweHorsthuis/machine-learning-cousera/blob/main/images/vectorization_notation.PNG?raw=true)  \n",
    "\n",
    "## Gradient descent for multiple features\n",
    "The gradient descent formula also changes for multiple features. As shown here:  \n",
    "![](https://github.com/DouweHorsthuis/machine-learning-cousera/blob/main/images/gradient_descent_vectorization.PNG?raw=true)  \n",
    "Here you can see that *w* and *x<sup>(i)</sup>* change.  \n",
    "\n",
    "## Alternative to gradient descent for linear regression\n",
    "It is called *Normal equation*. It only works for linear regression and is slow for large number. It doesn't use iterations. But is not recommended. \n",
    "\n",
    "## Features and parameter values  \n",
    "### The relation between parameters and value\n",
    "For the example we are using we look at the price of a house. We have 2 parametrs so the formula is:  \n",
    "$\\hat{price}$ = w<sub>1</sub>x<sub>1</sub>+w<sub>2</sub>x<sub>2</sub> +b.  \n",
    "In this case x<sub>1</sub> = size in sqft and x<sub>2</sub> = the amount of bedrooms. This would mean that x<sub>1</sub> would range between 300-2000 so it's a large range of numbers, while x<sub>2</sub> ranges from 1-5 so it's a small range of numbers. In this case it is likely for a good model to find that because of that w<sub>1</sub> is small and w<sub>2</sub> is large. So in a way w<sub>1</sub> and x<sub>1</sub> and x<sub>2</sub> and w<sub>2</sub> are the opposite of each other, where one is large the other is small or the opposite. The figure below explains the rest of the example:  \n",
    "![](https://github.com/DouweHorsthuis/machine-learning-cousera/blob/main/images/features_vs_parameters.PNG?raw=true)  \n",
    "  \n",
    "### What does this mean for gradient decent  \n",
    "When looking at the figure below. You see that the horizontal axis has a way larger scale compared to the vertical axis. Next to that you see that the cost plotted which is always a contour plot. and you see that it is a narrow plot, where the horizontal axis ranges from 0-1 but the vertical axis is way larger. This creates an oval shaped plot. This is because a small change on w<sub>1</sub> will have a large impact in contrast a small change in w<sub>2</sub> has few impact on the outcome.   \n",
    "![](https://github.com/DouweHorsthuis/machine-learning-cousera/blob/main/images/impact_gradient_decent.PNG?raw=true)  \n",
    "  \n",
    "### Feature scaling for better gradient decent  \n",
    "When you have parameters as described above, you might run into the problem that gradient descent will take a long time. This is because a small difference in on parameter might have such a big impact while the other makes few difference, that it's hard to find the global minimum.  \n",
    "You can see this in the two plots here at the top of the figure. \n",
    "![](https://github.com/DouweHorsthuis/machine-learning-cousera/blob/main/images/rescaling.PNG?raw=true)  \n",
    "However, if you would rescale your parameters to be similar to each other, let's say bot of them being a number between 0-1. The impact of both would be more equal and thus lead to a quicker path to the solution. ** your aim would be to get close to somewhere between -1 $\\le x_j \\le$ 1 for each feature $x_j$. This is not to say that if you have a rang of 0-3 or -2-0.5 that its wrong, but for example -100-100 pr -0.00001-0.0001 would be an issue. **when in doubt carry it out**, it's unlikely to cause an issue.\n",
    "  \n",
    "### How to Feature scale  \n",
    "One way of doing it is if you have a range from 300-2000. You can divide your feature by the max range (2000), this should give you a scale ranging from 0.15-1. See the table below for 2 examples:  \n",
    "\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "| Range |  300 $\\le$ x<sub>1</sub> $\\le$ 2000 | 0 $\\le$ x<sub>1</sub> $\\le$ 5 |  \n",
    "|--|--|--|  \n",
    "|Calculate the scaled x | x<sub>1, scaled</sub> = $\\frac{x_1}{2000}$ | x<sub>2, scaled</sub> = $\\frac{x_2}{5}$ |  \n",
    "| Scaled range | 0.15  $\\le$ x<sub>1,scaled</sub> $\\le$ 1 | 0  $\\le$ x<sub>2,scaled</sub> $\\le$ 1 | \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mean normalization  \n",
    "A different way to better gradient decent is to do mean normalization. For this you first need to get the mean or $\\mu$. In our example this is 600. You need to subtract this from $x_1$ and divide by the max range minus the minimum range (2000-300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Range |  300 $\\le$ x<sub>1</sub> $\\le$ 2000 | 0 $\\le$ x<sub>1</sub> $\\le$ 5 |  \n",
    "|--|--|--|  \n",
    "| | x<sub>1</sub> = $\\frac{x_1 - \\mu_1}{2000 -300}$ | x<sub>2, scaled</sub> =  $\\frac{x_2 - \\mu_2}{5 -0}$|  \n",
    "| Mean normalized range | -0.18  $\\le$ x<sub>1</sub> $\\le$ 0.82 | -0.46  $\\le$ x<sub>2</sub> $\\le$ 0.54 | "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Z-score normalization\n",
    "The 3rd way is to calculate the z-score normalization. For this you need to calculate the standard deviation or $\\sigma$. In our example the standard deviation for $\\sigma_1$ = 450 and our mean or $\\mu_1$ = 600. When you have calculated those, you take $x_1$ - $\\mu_1$ and subtract them by $\\sigma_1$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Range |  300 $\\le$ x<sub>1</sub> $\\le$ 2000 | 0 $\\le$ x<sub>1</sub> $\\le$ 5 |  \n",
    "|--|--|--|  \n",
    "| | x<sub>1</sub> = $\\frac{x_1 - \\mu_1}{\\sigma_1}$ | x<sub>2, scaled</sub> =  $\\frac{x_2 - \\mu_2}{\\sigma_2}$|  \n",
    "| Z-scored range | -0.67  $\\le$ x<sub>1</sub> $\\le$ 3.1 | -1.6  $\\le$ x<sub>2</sub> $\\le$ 1.9 | "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How can you find out if gradient decent is converting\n",
    "To recap, converting is important because this would mean that gradient decent has found the local minima.  \n",
    "  \n",
    "### Plotting the learning curve\n",
    "One of the ways to do it it so plot it. If you want to do that you plot J($\\overrightarrow{w}$,b) on the Y axis and the # iterations on the X axis. Like this you can see how quick and when it reaches the minimum. This curve is called the *learning curve*. If the learning rate increases after a single iteration, $\\alpha$ is either chosen poorly (too large), or a bug in the code. \n",
    "  \n",
    "### Automatic convergence test  \n",
    "Set $\\epsilon$ to a small number, for example 10<sup>-3</sup> (0.001). If J($\\overrightarrow{w}$,b) decreases by $\\le \\epsilon$ in one iteration, it declares convergence. *It will also let you know if gradient decent doesn't work well*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choosing the right learning rate  \n",
    "As mentioned before, the learning rate or $\\alpha$ has a big impact on gradient descent. The question that we will try to answer here is, how do you pick the right number?\n",
    "\n",
    "### $\\alpha$ is too large   \n",
    "If you plot the learning curve, and it goes down but also up some times, you have an issue. This is often caused by $\\alpha$ or the learning rate being too high. When $\\alpha$ is too high, it might overshoot the local minima. So instead of it going to the lowest point it will end up a little higher, and continue from there. Another possibility of what happens with a too large $\\alpha$ is that, the learning curve keeps increasing. Both of these possibilities can also be cause by different types of bugs in the code. One example of this could be if you use the formula $w_1 = w_1$ **+** $\\alpha d_1$ instead of $w_1 = w_1$ **-** $\\alpha d_1$.  \n",
    "  \n",
    "#### Spotting the bug  \n",
    "If $\\alpha$ increases, the easiest way to narrow down the problem is to simply decrease $\\alpha$ to a very small number. If it works now, $\\alpha$ was too big. If it doesn't work the problem is a bug in the code.  \n",
    "  \n",
    "### $\\alpha$ is too small  \n",
    "This is a less big problem, because your code should work. However it might take long (seemingly forever even). Because $\\alpha$ has such a big impact on the size of the learning steps, that it being really low means you need a lot of steps.  \n",
    "\n",
    "### What numbers to choose for $\\alpha$  \n",
    "It's good practice to start with a very low number like 0.001 and work your way up. This could mean roughly increasing $\\alpha$ by multiplying it by 3 **0.001 - 0.003 - 0.01 - 0.03 - 0.1 - 0.3 - 1**. When you plot the learning curve for these numbers (for just a hand full of iterations), you should look for the following:\n",
    "1. How quick is it going down (quick = better).  \n",
    "2. Is it always going down (going back up mean $\\alpha$ is too high)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature engineering  \n",
    "If you have a model, lets say: *$f$ <sub>$\\overrightarrow{w}$,b</sub>($\\overrightarrow{x}$)=$w_1 x_1 + w_2 x_2 +b$*.  \n",
    "Where $x_1$ is the frontage of a plot of land and $x_2$ the dept. It could make sense that a more sensitive feature would be the area (frontage X dept).  \n",
    "![](https://github.com/DouweHorsthuis/machine-learning-cousera/blob/main/images/house_plot.PNG?raw=true)  \n",
    "So you could create: $x_3 = X_1 X_2$ and turn the model into *$f$ <sub>$\\overrightarrow{w}$,b</sub>($\\overrightarrow{x}$)=$w_1 x_1 + w_2 x_2 + w_3 x_3 +b$*  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Polynomial regression  \n",
    "So far we have only looked at fitting straight lines to the data. But in reality this is probably not (always) the case. For example if you see the plot below here, you see that the it seems like the price stops increasing when you reach a certain size. In this case you might want to add a quadratic function (size x and size x squared) as you see here in pink. However, in that case the curve will also go back down. So maybe instead you want a cubic function, where you have x square and x cubed. You can see that in purple.  \n",
    "![](https://github.com/DouweHorsthuis/machine-learning-cousera/blob/main/images/Polynomial_regression.PNG?raw=true).  \n",
    "It's important to add here that because you take x, $x^2$ and $x^3$. Feature scaling becomes very important. Because the the scale is very different between the 3 numbers. \n",
    "  \n",
    "If you have the model  $y=w_0x_0 + w_1x_1^2 + w_2x_2^3+b$ you should get 3 numbers for w. These should give you an indication which of the x's were most impactful. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python vs algebra\n",
    "**Indexing**  \n",
    "In Python everything is 0 indexed whereas algebra (and Matlab) start at 1. This means for the vector $\\overrightarrow{w}$ = [w<sub>1</sub> w<sub>2</sub> w<sub>3</sub>] that in Python to get to the first parameter (w<sub>1</sub>), you would use w<sub>[0]</sub>  \n",
    "\n",
    "**Without vectorization** \n",
    "To run the formula  *f<sub>w,b</sub> (x)=w<sub>1</sub>x<sub>1</sub>+w<sub>2</sub>x<sub>2</sub>+w<sub>3</sub>x<sub>3</sub>+b* you would need to type out:  \n",
    "```python\n",
    "f = w[0] * x[0] +  \n",
    "    w[1] * x[1] +  \n",
    "    w[2] * x[2] + b \n",
    "```\n",
    "This is not handy, quick or easy to compute. Because of that you can instead use this formula (as mentioned above): *$f$ <sub>$\\overrightarrow{w}$,b</sub>($\\overrightarrow{x}$)= $\\sum\\limits_{j=1}^{n}$ w<sub>j</sub>x<sub>j</sub> + b*. This translates to the following code:  \n",
    "\n",
    "```python\n",
    "f = 0  \n",
    "for j in range (0,n): #often writen range(n)\n",
    "    f = f + w[j] * x[j]\n",
    "f = f + b\n",
    "```\n",
    "This is better on the typing front, however it is still not the best, on the computational front. For that you need to use the dot product or vectorization: *$f$ <sub>$\\overrightarrow{w}$,b</sub>($\\overrightarrow{x}$)=$\\overrightarrow{w}$ $\\cdot$ $\\overrightarrow{x}$ + b*. This translate to the following python code that requires the `numpy` library:  \n",
    "```python\n",
    "f = np.dot(w,x + b)\n",
    "``` "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ba80823ec9edfa2b915b0e8b60f600e1ca4cb90dcbbf7f0d0e0246605aecf2a6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
