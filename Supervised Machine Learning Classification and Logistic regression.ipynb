{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine learning \n",
    "**Classification and Logistic regression**  \n",
    "All the formulas below have the python code below them, to run it first import the correct library's and load the training data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Supervised learning\n",
    "\n",
    "In this case you give your algorithm examples to learn form. You do this by giving the right answer or **label**. This turns *input **X** -> output label **Y*** . To clarify you give examples of correct output Y labels.\n",
    "\n",
    "| Input X           | Output Y               | Application         |\n",
    "|-------------------|------------------------|---------------------|\n",
    "| email             | spam                   | spam filter         |\n",
    "| audio             | text transcript        | speech recognition  |\n",
    "| English           | Spanish                | machine translation |\n",
    "| ad, user info     | click (0/1)            | online advertising  |\n",
    "| image, radar info | position of other cars | self-driving car    |\n",
    "| Image of phone    | defect? (0/1)          | visual inspection   | "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Classification**\n",
    "\n",
    "Try to classify data into multiple possible outputs. In this case class of categories are used interchangeably and these are what is predicted by the algorithm. You can have multiple inputs. **The goal is to predict a category from a small number of possible outputs.** Linear regression is not a good algorithm for this problem. Because you can get infinitive numbers for the outcome and you want only 2. Furthermore, even when you would get a useful output to create a boundary that would separate well, adding more data would very quickly prove it to not be good anymore.  The goal of classification is often answered by Yes or no, in this case you call it Binary classification.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic regression\n",
    "Instead of a straight line (or any of the other linear options), logistic regression creates a kind of S shaped line. To calculate this one would use the **Sigmoid function** or logistic function. The sigmoid functions outputs values between 0 and 1. It has the formula g(z) = $\\frac{\\partial 1}{\\partial 1+e^{-z}}$. Another way people might write it is P(y=1|x;$\\overrightarrow{w}$,b). Here what they mean is the probability that y=1 given x with the parameters x and b. This should give you a number like 0.7 or 0.3. In the formula z is equal to $\\overrightarrow{w} \\cdot \\overrightarrow{x} + b$. This means we can write the full formula as $ f_{\\overrightarrow{w},b} (\\overrightarrow{x}) $ = g($\\overrightarrow{w} \\cdot \\overrightarrow{x} + b)$ = g(z) = $\\frac{\\partial 1}{\\partial 1+e^{-z}}$ P(y=1|x;$\\overrightarrow{w}$,b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Threshold\n",
    "But how do you get a prediction if it's a 1 or a 0 (classifier). In this case you need to set a threshold. Since the outcome of the formula is a number between 0 and 1. 0.5 would make a lot of sense because this would be half way. But what would this mean.  \n",
    "$\\hat{y} $= 1 if  \n",
    "$ f_{\\overrightarrow{w},b} (\\overrightarrow{x}) \\le 0.5 $  \n",
    "g(z) $\\le 0.5 $  \n",
    "z $\\le 0.5 $  \n",
    "$\\overrightarrow{w} \\cdot \\overrightarrow{x} + b  \\le 0.5 $  \n",
    "\n",
    "However, while 0.5 makes sense when the data is split in a linear way, it might not make sense when the data is very complex. So ultimately finding a threshold should be based on your data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cost function for Logistic regression  \n",
    "#### Why not use the squared error cost function\n",
    "We already went over the squared error cost function for linear regression. Where $f_{\\overrightarrow{w},b} (\\overrightarrow{x}) \\overrightarrow{w} \\cdot \\overrightarrow{x} + b $. This cost function would however not work for logistic regression. If we would try it for logistic regression the function would be:  \n",
    "$ f_{\\overrightarrow{w},b} (\\overrightarrow{x}) $ = $\\frac{\\ 1}{\\ 1+e^{-z}}$. This would be instead a non-convex function. Which would create a lot of local minima which would be a problem.  \n",
    "\n",
    "![The problem of using squared cost error for logistic regression](https://github.com/DouweHorsthuis/machine-learning-cousera/blob/main/images/square_cost_error_logistic_regression.PNG?raw=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loss function\n",
    "The *loss* is the part of the cost function that comes after the sum:  \n",
    "*J($\\overrightarrow{w}$,b)=$\\frac{1}{m}$ $\\sum\\limits_{i=1}^{m}$ $\\frac{1}{2}$ ($f$<sub>$\\overrightarrow{w}$,b</sub>($\\overrightarrow{x}$<sup>(i)</sup>)-y<sup>(i)</sup>)<sup>2</sup>*.  \n",
    "To be clear, the original cost function is slightly different because we moved the 1 half inside the summation. This is to make the math easier later on. So from here on we will call loss:  \n",
    "L($f$<sub>$\\overrightarrow{w}$,b</sub>($\\overrightarrow{x}$<sup>(i)</sup>),y<sup>(i)</sup>)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loss for logistic regression  \n",
    "Now that we know that the cost function doesn't work for logistic regression, we need to adjust it so that it will. For this we will specifically look at the loss part. Instead of using L($f$<sub>$\\overrightarrow{w}$,b</sub>($\\overrightarrow{x}$<sup>(i)</sup>),y<sup>(i)</sup>)  we need to re-define it.  \n",
    "To work we will actually have 2 versions, one for a prediction of $y^{(i)}=1$ and one when $y^{(i)}=0$  \n",
    "\n",
    "if $y^{(i)}=1$ we use   -log($f$<sub>$\\overrightarrow{w}$,b</sub>($\\overrightarrow{x}$<sup>(i)</sup>))  \n",
    "if $y^{(i)}=0$ we use   -log(1 - $f$<sub>$\\overrightarrow{w}$,b</sub>($\\overrightarrow{x}$<sup>(i)</sup>))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Simplified loss function    \n",
    "If we use the new loss part, the cost function will become convex again. The whole function would look like this:  \n",
    "$$loss(f_{\\mathbf{\\overrightarrow{w}},b}(\\mathbf{x}^{(i)}), y^{(i)}) = (-y^{(i)} \\log\\left(f_{\\mathbf{w},b}\\left( \\mathbf{x}^{(i)} \\right) \\right) - \\left( 1 - y^{(i)}\\right) \\log \\left( 1 - f_{\\mathbf{w},b}\\left( \\mathbf{x}^{(i)} \\right) \\right)$$  \n",
    "\n",
    "And to clarify, if $y^{(i)}$ = 0 the left hand side is eliminated, and if  $y^{(i)}$ = 1 the right hand side is eliminated. Which makes it a little easier to work with. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Regularization\n",
    " we go from:  \n",
    "$J(\\overrightarrow{w},b) = -\\frac{1}{m} \\sum\\limits_{i=1}^{m} \\left[ y^{(i)} \\log\\left(f_{{\\overrightarrow{w}},b}\\left( \\overrightarrow{x}^{(i)} \\right) \\right) + \\left( 1 - y^{(i)}\\right) \\log \\left( 1 - f_{\\overrightarrow{x},b}\\left( \\overrightarrow{x}^{(i)} \\right) \\right) \\right]$  \n",
    "\n",
    "To:  \n",
    "$J(\\overrightarrow{w},b) = -\\frac{1}{m} \\sum\\limits_{i=1}^{m} \\left[ y^{(i)} \\log\\left(f_{{\\overrightarrow{w}},b}\\left( \\overrightarrow{x}^{(i)} \\right) \\right) + \\left( 1 - y^{(i)}\\right) \\log \\left( 1 - f_{\\overrightarrow{x},b}\\left( \\overrightarrow{x}^{(i)} \\right) \\right) \\right] $<ins>$\\frac{\\lambda}{m}w_j$</ins> $  \n",
    "  \n",
    "Which mean for the derivatives:  \n",
    "$w_j = \\frac{1}{m} \\sum\\limits_{i=1}^{m} [(f_{\\overrightarrow{w},b})(\\overrightarrow{x}^{(i)})-y^{(i)})x^{(i)}_j]+$<ins>$\\frac{\\lambda}{m}w_j$</ins>  \n",
    "  \n",
    "$b=\\frac{1}{m} \\sum\\limits_{i=1}^{m} [(f_{\\overrightarrow{w},b})(\\overrightarrow{x}^{(i)})-y^{(i)})$  \n",
    "*which both need to be updated simultaneously*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Terminology\n",
    "\n",
    "| Definition        | Explanation            |\n",
    "|-------------------|------------------------|\n",
    "| Classification| A model where output Y can only take on 1 out of a small number of values |  \n",
    "| Binary classification | If y can only be one of two values | \n",
    "| Class or category | Often used interchangeably, they are the answers to the classification | \n",
    "|Answers Yes/True/1 | Positive class|\n",
    "|Answers No/False/0 | Negative class|  \n",
    "|Sigmoid Function or Logistic Function | g(z) = $\\frac{\\partial 1}{\\partial 1+e^{-z}}$ |\n",
    "| Loss * $loss(f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}), y^{(i)})$ | the difference of every example to it's target | \n",
    "| Cost | All the losses together over the dataset |  \n",
    "| Underfit / High bias | A model that does not fit the data well |\n",
    "| Generalization | How well the model works on new data | \n",
    "| Overfit / High variance | A model that fits the training data perfectly, but does not generalize well to new data |  \n",
    "| Regularization | Reducing a parameter but not setting it to 0 |"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ba80823ec9edfa2b915b0e8b60f600e1ca4cb90dcbbf7f0d0e0246605aecf2a6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
