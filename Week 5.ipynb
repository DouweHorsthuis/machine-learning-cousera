{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Specialization\n",
    "## Advanced learning Algorithms\n",
    "# Week 5  \n",
    "  \n",
    "## Terminology\n",
    "\n",
    "| Definition        | Explanation            |\n",
    "|-------------------|------------------------|\n",
    "| **ReLU** (Rectified Linear Unit)| $g(z)=max(0,z)$ |\n",
    "| Multiclass (Classification problem)| A classification problem where you can have more outcomes than 0/1. | \n",
    "|Softmax regression algorithm | a generalization of logistic regression which is a binary classification algorithm to the multiclass classification contexts|  \n",
    "|Softmax layer | if the output layer is using softmax |  \n",
    "|Adam (**Ad**aptive **M**oment **E**stimation) | This algorithm can adapt the learning rate $\\alpha$|  \n",
    "|Convolutional Layer| A layer of a neural network that only focuses on a specific input |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training steps  \n",
    "The summaries the previous knowledge, lets compare the basic 3 steps that are quite similar between logistic regression and a neural network. \n",
    "|step #| explanation | formula  | Logistic Regression | Neural network (Tensor flow) | \n",
    "|--|--|--|--|--|\n",
    "|Step 1| specify how to compute output <br/> given input x and parameters w,b | $f_{\\overrightarrow{w},b} (\\overrightarrow{x}) $ = g($\\overrightarrow{w} \\cdot \\overrightarrow{x} + b)$ = g(z) | z= np.dot(w,b)+b  <br/> f_x= 1/(1+np.exp(-z)) | model = Sequential([ <br/> Dense (...) <br/> Dense (...) <br/> Dense (...) <br/>])  \n",
    "|Step2| Specify the loss and cost function | $L(f_{\\overrightarrow{w},b}(\\overrightarrow{x}),y)$ <br/> Cost=J($\\overrightarrow{w},b)=\\frac{1}{m} \\sum\\limits_{i=1}^{m} L(f$<sub>$\\overrightarrow{w}$,b</sub>($\\overrightarrow{x}$<sup>(i)</sup>),y<sup>(i)</sup>)  | loss = -y * np.log(f_x) <br/> -(1-y * np.log(1-f_x)) |model.compile( <br/> loss=BinaryCrossentropy()) |\n",
    "|Step 3| Train on data by using gradient decent. | Minimize $J(\\overrightarrow{w},b)$ | w=w-alpha*dj_dw <br/> b= b-alpha * dj_db | model.fit(X,y,epochs=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a neural network (code)  \n",
    "The following code can be used to build a neural network:  \n",
    "### Step 1 import tensorflow and numpy\n",
    "```python\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### step 2  create the model  \n",
    "The model below will create a neural network that would look something like this:  \n",
    "![](https://github.com/DouweHorsthuis/machine-learning-cousera/blob/main/images/neural_network_2.PNG?raw=true)   \n",
    "```python\n",
    "model = Sequential(\n",
    "    [\n",
    "        tf.keras.Input(shape=(2,)),\n",
    "        Dense(25, activation='relu', name = 'layer1'), \n",
    "        Dense(15, activation='relu', name = 'layer2')  \n",
    "        Dense(1, activation='sigmoid', name = 'layer2')  # activation='linear' is the 3rd option \n",
    "     ]\n",
    ")\n",
    "``` "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### step 3 loss and cost function\n",
    "For the binary classification problem the formula we use is:  \n",
    "$L(f\\overrightarrow{x},y)=-ylog(f(\\overrightarrow{x}))-(1-y)log(1-f(\\overrightarrow{x}))$(it the same we use for logistic regression).  \n",
    "This is called BinaryCrossentropy in Tensorflow. In runs with this line of code:  \n",
    "```python\n",
    "from tensorflow.keras.losses import BinaryCrossentropy \n",
    "model.compile(loss=BinaryCrossentropy())\n",
    "```  \n",
    "However, if you have a different problem, for example regression problem. You want to use a different model, where you want to reduce the square error loss ywhich would be this formula:  \n",
    "$J(W,B)=\\frac{1}{m} \\sum\\limits_{i=1}^{m} L(f(\\overrightarrow{x}^{(i)}),y{(i)})$  \n",
    "To clarify, we use capital W and capital B to represent all the W and B parameters in your neural network.  This can be represented with the code:  \n",
    "```python\n",
    "from tensorflow.keras.losses import MeanSquareError \n",
    "model.compile(loss=MeanSquareError())\n",
    "```  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4 Gradient descent\n",
    "Finally you want to minimize the cost function using gradient descent. You would do that using the following formula:  \n",
    "repeat{  \n",
    "&nbsp; &nbsp; &nbsp;$w^{[l]}_j =w^{[l]}_j-\\alpha\\frac{d}{dw_j} J(\\overrightarrow{w},b)$  \n",
    "&nbsp; &nbsp; &nbsp;$b^{[l]}_j =w^{[l]}_j-\\alpha\\frac{d}{db} J(\\overrightarrow{w},b)$  \n",
    "&nbsp;}  \n",
    "  \n",
    "Your code to run this in Tensorflow would be \n",
    "```python\n",
    "model.fit(X,y,epochs=100)\n",
    "```  \n",
    "where the amount of epoch represent the steps that gradient descent should take."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why do we need activation functions?\n",
    "If every neuron would not use a activation function, the whole neural network would be one big linear regression. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Alternatives to the sigmoid activation  \n",
    "In all the previous examples we used the sigmoid function to calculate the activation of a neuron. To recap, in formula:  \n",
    "$a^{[1]}_2 = g(\\overrightarrow{w}^{[1]}_2 \\cdot \\overrightarrow{x} +b{[1]}_2)$  \n",
    "The $\\overrightarrow{w}^{[1]}_2 \\cdot \\overrightarrow{x} +b{[1]}_2)$ part is z and can also be calculated as $g(z)= \\frac{1}{1+e^{-z}}$ This give us a number between 0-1.  \n",
    "![](https://github.com/DouweHorsthuis/machine-learning-cousera/blob/main/images/sigmoid.PNG?raw=true)    \n",
    "However, this might not be right for all neurons. A often used different way to calculate Z is $g(z)max(0,z)$ This one is called **ReLU** (Rectified Linear Unit).  \n",
    "![](https://github.com/DouweHorsthuis/machine-learning-cousera/blob/main/images/ReLU.PNG?raw=true)   \n",
    "And sometimes you can use the **linear activation function** $g(z)=z$. In this case some people might say you are using no activation function. Because this makes the formula ($a^{[1]}_2 = g(\\overrightarrow{w}^{[1]}_2 \\cdot \\overrightarrow{x} +b{[1]}_2)$ ) as if there is not g in it at all.  \n",
    "![](https://github.com/DouweHorsthuis/machine-learning-cousera/blob/main/images/linear.PNG?raw=true)  \n",
    "  \n",
    "\n",
    "### How do you choose which one you use?  \n",
    "Since you can choose different activation functions for different neurons, how do you know which one to choose?  \n",
    "#### Output layer\n",
    "Here your choice is somewhat simple, because it depends on what you want your outcome to be. If it is a binary classification, you want a 0-1 number, so you want to use a **sigmoid activation function** (y=0/1).  If your outcome should be for example a prediction of stock markets, then the output could be positive or negative, so here you would want to use the **linear activation function**(y=+/-). If you want the outcome toe be something like the cost of a house, you know it cannot be a negative number, so you would choose the **ReLU activation function**(y=0/+).  \n",
    "#### Hidden layers\n",
    "ReLU seems to be the most common choice by most people now-a-days. This is because because ReLU is a bit faster to compute. The second, and more important, reason is that it doesn't go flat in 2 places like the Sigmoid function. Which makes it better (quicker) for gradient decent. Using the linear function would be wrong. This would turn a neural network into a big linear model and defeat it's purpose."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### New activation functions\n",
    "Every few years research comes up with a new function. This can indeed work better than whatever we currently use. So it's important to keep up with the field. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiclass classification problem  \n",
    "This is the case when you have a classification problem where you don't want to know if x = class 1 or if x $\\neq$ class 1. But instead you would want to know the probability between multiple classes. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Softmax regression algorithm  \n",
    "The softmax regression algorithm is a generalization of logistic regression, which is a binary classification algorithm to the multiclass classification contexts.\n",
    "#### Logistic regression  \n",
    "To think about this, we first have to think about Logistic regression, where we are looking for 2 possible output values. In that case we would use $z=\\overrightarrow{w}\\cdot\\overrightarrow{x} +b$ This allows you to compute $a_1=g(z)=\\frac{1}{1+e^{-z}}$ Which gives us the probability of y being 1 ($P(y=1|\\overrightarrow{x}))$. Importantly if you would know that that for example P=0.75, you would also know that there is a 25% chance that y is equal to zero.  \n",
    "#### Softmax regression  \n",
    "For a Softmax regression with 4 possible outputs (y=1,2,3,4) you will need to calculate 4 things:  \n",
    "$z_1=\\overrightarrow{w}_1\\cdot\\overrightarrow{x} +b_1$  \n",
    "$z_2=\\overrightarrow{w}_2\\cdot\\overrightarrow{x} +b_2$  \n",
    "$z_3=\\overrightarrow{w}_3\\cdot\\overrightarrow{x} +b_3$  \n",
    "$z_4=\\overrightarrow{w}_4\\cdot\\overrightarrow{x} +b_4$  \n",
    "  \n",
    "The formula for Softmax Regression would be this:  \n",
    "$a_1=\\frac{e^{z_{1}}}{e^{z_{1}}+e^{z_{2}}+e^{z_{3}}+e^{z_{4}}}=P(y=1|\\overrightarrow{x}) \n",
    "$a_2=\\frac{e^{z_{2}}}{e^{z_{1}}+e^{z_{2}}+e^{z_{3}}+e^{z_{4}}}P(y=2|\\overrightarrow{x})$  \n",
    "$a_1=\\frac{e^{z_{3}}}{e^{z_{1}}+e^{z_{2}}+e^{z_{3}}+e^{z_{4}}}P(y=3|\\overrightarrow{x})$   \n",
    "$a_1=\\frac{e^{z_{4}}}{e^{z_{1}}+e^{z_{2}}+e^{z_{3}}+e^{z_{4}}}P(y=4|\\overrightarrow{x})$  \n",
    "  \n",
    "**The generalization of these formulas for N possible outputs (y=1,2,3,...,N)**  \n",
    "$z_j=\\overrightarrow{w}_j\\cdot\\overrightarrow{x} +b_j$  \n",
    "$a_j=\\frac{e^{z_{j}}}{\\sum_{k=1}^{N}e^{z_{k}}}=P(y=j|\\overrightarrow{x}) $ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Softmax cost and loss\n",
    "The loss function is also a little different, due to $N$ \n",
    "\n",
    "$\\begin{equation}\n",
    "  L(\\mathbf{a},y)=\\begin{cases}\n",
    "    -log(a_1), & \\text{if y=1}.\\\\\n",
    "        &\\vdots\\\\\n",
    "     -log(a_N), & \\text{if y=N}\n",
    "  \\end{cases}\n",
    "\\end{equation}$  \n",
    "\n",
    "This in turn creates the full cost function to look like this:  \n",
    "$$\\begin{align}\n",
    "J(\\mathbf{w},b) = -\\frac{1}{m} \\left[ \\sum_{i=1}^{m} \\sum_{j=1}^{N}  1\\left\\{y^{(i)} == j\\right\\} \\log \\frac{e^{z^{(i)}_j}}{\\sum_{k=1}^N e^{z^{(i)}_k} }\\right] \\tag{2}\n",
    "\\end{align}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tensorflow code \n",
    "THe following code should create a neural network, using both relu and softmax for a 3 layered networks where layer 1 has 25 units, layer 2 has 15 and the last layer has 10:  \n",
    "```python\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense  \n",
    "from tensorflow.keras.losses import SparseCategoricalCrossentropy \n",
    "model = Sequential(\n",
    "    [\n",
    "        tf.keras.Input(shape=(2,)),\n",
    "        Dense(25, activation='relu'), \n",
    "        Dense(15, activation='relu')  \n",
    "        Dense(10, activation='softmax')  \n",
    "     ]\n",
    ")  \n",
    "model.compile(loss=SparseCategroicalCrossEntropy()) \n",
    "model.fit(X,Y,epochs=100)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rounding problem\n",
    "Below you see 2 ways a computer can caluculate 2.0/10000, surprisingly it gives us 2 different numbers. This has to do with rounding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x1=0.000200000000000000\n",
      "x2=0.000199999999999978\n"
     ]
    }
   ],
   "source": [
    "x1=2.0/10000\n",
    "print(f\"x1={x1:.18f}\") \n",
    "x2= 1+(1/10000)-(1-1/10000)\n",
    "print(f\"x2={x2:.18f}\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This can be a bit of an issue when using softmax, because it relies on mulitple loss calculations (more than lets say a logisitc regression). Because of this it is better to use the following modified code:  \n",
    "#### Tensorflow code (needs to be updated because there is better code)\n",
    "THe following code should create a neural network, using both relu and softmax for a 3 layered networks where layer 1 has 25 units, layer 2 has 15 and the last layer has 10:  \n",
    "```python\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense  \n",
    "from tensorflow.keras.losses import SparseCategoricalCrossentropy \n",
    "model = Sequential(\n",
    "    [\n",
    "        tf.keras.Input(shape=(2,)),\n",
    "        Dense(25, activation='relu'), \n",
    "        Dense(15, activation='relu')  \n",
    "        Dense(10, activation='linear')  #not softmax anymore\n",
    "     ]\n",
    ")  \n",
    "model.compile(loss=SparseCrossEntropy(from_logits=True)) #this is the fix for rounding issues.\n",
    "model.fit(X,Y,epochs=100)\n",
    "## adding these 2 lines to get a predicition\n",
    "logits=model(X)\n",
    "f_x=tf.nn.softmax(logits)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, instead of getting $a_1...a_10$ we get $z_1...z_10$ This is why the last 2 lines will turn that back into the predictions that you were looking for."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi label classification    \n",
    "A multi label classification can be useful if you want to know several things of your input. For example, if you have a picture as an input and you want to know for 3 things if they are happening or not. This means that your output layer needs to have 3 units that can have a sigmoid function to give you a probability for each of these questions. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Alternatives to Gradient Descent (most used now)  \n",
    "The formula for Gradient Descent=  \n",
    "$w_j=w_j-\\alpha \\frac{d}{dw_j}J(\\overrightarrow{w},b) $  Where $\\alpha$ is the learning rate. THe problem with $\\alpha$ is that during the first part you would want it to be big, so gradient descent takes big steps towards the minimum, but the opposite is true for when you get close to it. Since you don't want to \"overshoot\" the local minimum, you want $\\alpha$ to be small.   \n",
    "**Adam Algorithm Intuition** adjust $\\alpha$. Adam, or **Ad**aptive **M**oment **E**stimation, will not just use one $\\alpha$, but instead use a new one for each feature + b. The idea is that $w_j$ or b keep moving in the same direction, increase $\\alpha_j$ and the opposite for when $w_j$ or b keep oscillating decrease $\\alpha_j$.  \n",
    "  \n",
    "This can be implemented by adding the \"adam optimizer\" to your code. You do this by updating your `model.compile` function:  \n",
    "```python\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3),loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)) \n",
    "```  \n",
    "It is worth to note that you should try a couple of learning rates, to see which one works fastest."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Different types of layers  \n",
    "While for now we focused on the \"dense\" layer, where every neuron/unit gets the same input and will give an input for all the following neurons/units.  \n",
    "![](https://github.com/DouweHorsthuis/machine-learning-cousera/blob/main/images/network_connection.PNG?raw=true)  \n",
    "A Convolutional Layer works differently. Instead all neurons/units focusing on all the input, they will all process their individual part of the input. This can be faster, and it might need less training data and is last prone to overfitting. These inputs can overlap between neurons but don't need to. You can also have several convolutional layers. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ba80823ec9edfa2b915b0e8b60f600e1ca4cb90dcbbf7f0d0e0246605aecf2a6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
