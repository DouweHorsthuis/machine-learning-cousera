{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced learning Algorithms\n",
    "# Week 4  \n",
    "  \n",
    "## Terminology\n",
    "\n",
    "| Definition        | Explanation            |\n",
    "|-------------------|------------------------|\n",
    "| Neural Networks | networks based on the brain | \n",
    "| Deep learning | building neural networks, now-a-days not perse focusing on recreating the brain|   \n",
    "| a |  \"Activation\" or outcome of a neural network model |  \n",
    "| layer | a grouping of, or single neuron that take the same or similar features and output a few numbers together |\n",
    "| input layer | the layer that inputs the data |  \n",
    "| hidden layer | layer(s) between the input and output layer, that the model will train on but won't give you an output for | \n",
    "| output layer | the layer that give the final output |\n",
    "| multilayer perceptron | A neural networks with multiple hidden layers | \n",
    "| neuron or unit | the parts of the layer that will do the calculation (1 layer can have multiple)| \n",
    "\n",
    "### Neural networks\n",
    "Neural networks or \"artificial neural networks\"  are networks based on how the actual human brain works. Because the (human) brain is more complex than any system we have been able to build, people got motived to use this as a sort of blue print to work from or towards. This started in the 1950 but stopped and resumed in the 80's to the 90's and started again around 2005. In 2005 it got re-branded to **Deep learning**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How does a neural network work?  \n",
    "As an example, let say you are selling t-shirts and you want to know if a new t-shirt will be a top seller or not. Which could lead to buying better inventory and better marketing campaigns.  You want to create a model in which you can input only a few numbers, like price/shipping cost/ marketing/materials, and give you a probability of it being a top seller. This is called the input layer. Now let's say the outcome of it being a top seller has to do with affordability/awareness/perceived quality. These will be called the activations.  You could create a \"neuron\" that uses price and shipping cost to model if people will think it's affordable. Now let's say awareness is mainly marketing, so you model a different \"neuron\" on that. Finally, you use price and material to create another \"neuron\" that tells you something about the perceived quality.   \n",
    "These 3 \"neurons\" for what is called a \"hidden layer\" because the model trains them on which and how much of a certain input it should rely on. The hidden part of the name comes from the fact that we don't get to see this. Whereas we do know the input layer and what is later put out.  \n",
    "This hidden layer of 3 \"neurons\", that will send their numbers to a $4^rd$ \"neuron\" (the output layer) that will calculate the probability of it being a top seller. This is also called the activation.  \n",
    "In this example, we decided which features from the input layer would specifically give input to which \"neuron\" in the first layer. This would be to time consuming to do in a big neural network. Because of that everything is connected to everything. It should learn which features are most important for.  \n",
    "All and all this seems similar to logistic regression, however, the main difference is that in the hidden layer it creates it's own features that should be better and works with those. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://github.com/DouweHorsthuis/machine-learning-cousera/blob/main/images/diagram_1.PNG?raw=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Face recognition example  \n",
    "As an example we can look at a face recognition algorithm. Let's say you want to be able to input a picture and output the identity of a person. First you need to think of what a picture really is. For a computer, a picture is a square of pixels. Let's say 1000x1000 pixels. All the pixels can be translated to a number, let's call it light intensity level. You can now transfer these numbers into a vector. This vector $\\overrightarrow{x}$ will have the size of 1 by 1 million (1000pixels X 1000 pixels).  \n",
    "So the picture is the input layer or $\\overrightarrow{x}$ which is fet to the first hidden layer, which get's fet into multiple other hidden layers to finally be fet into the output layer and give you your answer. You can use this to look into what each hidden layer is actually learning. People have figured out for example, that it's possible that one neuron in the first hidden layer might be looking for vertical lines, a second maybe for horizontal lines, a third for lines at 45 degrees etc. In the next hidden layer, individual neurons might look for part of a face, which would be grouped parts of the first hidden layer. The last hidden layer might look at whole faces and compare these. It seems that these classification algorithms follow a similar pattern, where they start small and end up look for bigger and bigger groups of shapes/parts of images and finally full images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing the math of a neural network  \n",
    "Let say we have a simple one neural network with one hidden layer. This would mean that we have an input $\\overrightarrow{x}$, that goes into the hidden layer. Lets say this has 3 neurons. So it outputs a vector with 3 numbers that go to the output layer, which does it's final calculation and we get it's output. This network could be visualized like this:  \n",
    "![](https://github.com/DouweHorsthuis/machine-learning-cousera/blob/main/images/simple_neural_network.PNG?raw=true)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this example we would have a vector $\\overrightarrow{x}$ going into each of the 3 neurons in the first hidden layer.  \n",
    "The first layer will do the following:  \n",
    "$\\overrightarrow{w}_1, b_1 $ &nbsp; &nbsp; &nbsp; $a_1=g(\\overrightarrow{w}_1 \\cdot \\overrightarrow{x} + b_1)$ where $g(z) = \\frac{\\partial 1}{\\partial 1+e^{-z}}$  \n",
    "The second neuron will do the same:  \n",
    "$\\overrightarrow{w}_2, b_2 $ &nbsp; &nbsp; &nbsp; $ a_2=g(\\overrightarrow{w}_2 \\cdot \\overrightarrow{x} + b_2)$  \n",
    "And the third neuron too:  \n",
    "$\\overrightarrow{w}_3, b_3 $ &nbsp; &nbsp; &nbsp; $ a_3=g(\\overrightarrow{w}_3 \\cdot \\overrightarrow{x} + b_3)$  \n",
    "  \n",
    "Because this is layer 1*, and other models might have a lot of hidden layers, you can add $^{[N layer]}$. To exemplify the first neuron (but you should do it for all):  \n",
    "$\\overrightarrow{w}_1^{[1]}, b_1^{[1]} a_1^{[1]}=g(\\overrightarrow{w}_1^{[1]} \\cdot \\overrightarrow{x} + b_1^{[1]})$    \n",
    "  \n",
    "**Generalized** this turns into:  \n",
    "$a_j^{[l]}=g (\\overrightarrow{w}_j^{[l]} \\cdot \\overrightarrow{a}_j^{[l-1]}+ b_j^{[l]})$  \n",
    "  \n",
    "*the first hidden layer is layer 1, but you can refer to the input layer as layer 0. When explaining how many layers a neural network has, this layer is not included. So a 4 layer networks has 3 hidden layers and output layer and of course a not counted input layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example you end up with 3 numbers after $layer^[1]$. These become vector $a^{[1]}$. This will be the input for $Layer[2]$ or the output layer. This layer will use:  \n",
    "$\\overrightarrow{w}_1^{[2]}, b_1^{[2]} $ &nbsp; &nbsp; &nbsp; $a_1^{[2]}=g(\\overrightarrow{w}_1^{[2]} \\cdot \\overrightarrow{a}^{[1]} + b_1^{[2]}) = a^{[2]}$.   \n",
    "After this you can use this final output to say something like is $a^{[2]} \\ge 0.5$? if yes $\\hat{y} = 1$ if not $\\hat{y} = 0$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Coding the neural networks\n",
    "One of the most used frameworks for this is [Tensorflow](https://www.tensorflow.org/). Using tensorflow it is relatively easy to build a network. I had to [install](https://www.tensorflow.org/install/pip?hl=nl#windows) it, which I did using the Anaconda Prompt (first \"conda install -c conda-forge cudatoolkit=11.2 cudnn=8.1.0\", then \"pip install tensorflow\").\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "x=np.array([[200.0, 17.0]])  #your input\n",
    "layer_1=Dense(units=3, activation='sigmoid') #creating a function for layer_1\n",
    "a1=layer_1(x)#using function you just created on your input\n",
    "layer_2 = Dense(units=1,activation='sigmoid')#creating a function for layer_2\n",
    "a2=layer_2(a1) #the last step\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It might be good to be aware that there is a difference between a Tenserflow array and a numpy array. However, if you want `a2` to be a numpy array at the end, you can simply use `a2.numpy()` and it will be back to being a numpy arrary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want Tensorflow to go through all your layers without having to do it manual, that can be done. In that case you would use `Sequential` which is a function where you can input all your layers. After that you run `model.compile(...)` it defines a loss function and specifies a compile optimization.\n",
    "  \n",
    "After that you run `model.fit(x,y)`. This will fit the data to the model using gradient decent. Once this is done, you can use `model.predict(x_new)` to predict the out come for a new input (`x_new`), "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extra steps you can take\n",
    "This could should do the trick, but you might want to add extra steps to tread your data a little better. For example, you might want to **normalize** your data. You can do this by running:  \n",
    "```python\n",
    "norm_l = tf.keras.layers.Normalization(axis=-1)\n",
    "norm_l.adapt(X)  # learns mean, variance\n",
    "Xn = norm_l(X)\n",
    "```  \n",
    "Another good practice is to when you use the `model.compile` function, to add the shape. This can be omited, which will result in Tensorflow calculating it later.  \n",
    "```python\n",
    "model = Sequential(\n",
    "    [\n",
    "        tf.keras.Input(shape=(2,)),\n",
    "        Dense(3, activation='sigmoid', name = 'layer1'), #3 units/neurons, layer 1, using sigmoid as we have done before\n",
    "        Dense(1, activation='sigmoid', name = 'layer2')  #1 unit/neuron, layer 2, using sigmoid as we have done before\n",
    "     ]\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After that, you want to choose your cost function. you can do this like this: \n",
    "\n",
    "```python\n",
    "from tensorflow.keras.losses import BinaryCrossentropy\n",
    "model.complile(loss=BinaryCrossentropy())\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally you want to fit the model. This is done like this:  \n",
    "```python\n",
    "model.fit.(X,Y,epochs=100)\n",
    "```\n",
    "The epochs are the amount of steps in gradient descent"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ba80823ec9edfa2b915b0e8b60f600e1ca4cb90dcbbf7f0d0e0246605aecf2a6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
